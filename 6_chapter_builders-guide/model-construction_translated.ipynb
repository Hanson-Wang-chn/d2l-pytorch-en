{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53fef961",
   "metadata": {
    "origin_pos": 1
   },
   "source": [
    "# 层和模块\n",
    ":label:`sec_model_construction`\n",
    "\n",
    "当我们首次介绍神经网络时，\n",
    "我们专注于具有单个输出的线性模型。\n",
    "在这里，整个模型仅由一个神经元组成。\n",
    "请注意，单个神经元\n",
    "（i）接收一组输入；\n",
    "（ii）生成相应的标量输出；\n",
    "（iii）具有一组可以更新的参数以优化感兴趣的某个目标函数。\n",
    "然后，当我们开始考虑具有多个输出的网络时，\n",
    "我们利用向量化算术来描述整个神经元层。\n",
    "就像单个神经元一样，\n",
    "层（i）接收一组输入，\n",
    "（ii）生成相应的输出，\n",
    "（iii）由一组可调参数描述。\n",
    "当我们通过softmax回归工作时，\n",
    "单层本身就是模型。\n",
    "然而，即使我们随后\n",
    "引入了多层感知机（MLPs），\n",
    "我们仍然可以认为模型保留了这种基本结构。\n",
    "\n",
    "有趣的是，对于MLPs，\n",
    "整个模型及其构成的各层\n",
    "都共享这种结构。\n",
    "整个模型接收原始输入（特征），\n",
    "生成输出（预测），\n",
    "并拥有参数\n",
    "（来自所有构成层的组合参数）。\n",
    "同样，每个单独的层接收输入\n",
    "（由前一层提供）\n",
    "生成输出（后续层的输入），\n",
    "并拥有一组根据从后续层反向流动的信号进行更新的可调参数。\n",
    "\n",
    "\n",
    "虽然你可能认为神经元、层和模型\n",
    "为我们提供了足够的抽象来进行我们的工作，\n",
    "但实际上我们经常发现方便地谈论比单个层大\n",
    "但比整个模型小的组件。\n",
    "例如，ResNet-152架构，\n",
    "在计算机视觉中非常流行，\n",
    "拥有数百层。\n",
    "这些层由重复的*层组*模式组成。逐层实现这样的网络可能会变得乏味。\n",
    "这种担忧不仅仅是假设性的——这种设计模式在实践中很常见。\n",
    "上面提到的ResNet架构\n",
    "赢得了2015年的ImageNet和COCO计算机视觉竞赛\n",
    "在识别和检测方面 :cite:`He.Zhang.Ren.ea.2016`\n",
    "并且仍然是许多视觉任务的首选架构。\n",
    "类似地，在其他领域，\n",
    "包括自然语言处理和语音，\n",
    "将层按各种重复模式排列的架构现在也无处不在。\n",
    "\n",
    "\n",
    "为了实现这些复杂的网络，\n",
    "我们引入了神经网络*模块*的概念。\n",
    "模块可以描述单个层，\n",
    "由多个层组成的组件，\n",
    "或整个模型本身！\n",
    "使用模块抽象的一个好处是它们可以组合成更大的构件，\n",
    "通常是递归的。这在:numref:`fig_blocks`中进行了说明。通过定义代码来按需生成任意复杂度的模块，\n",
    "我们可以编写出令人惊讶的紧凑代码\n",
    "同时实现复杂的神经网络。\n",
    "\n",
    "![多个层被组合成模块，形成更大模型的重复模式。](../img/blocks.svg)\n",
    ":label:`fig_blocks`\n",
    "\n",
    "\n",
    "从编程的角度来看，模块由*类*表示。\n",
    "它的任何子类都必须定义一个前向传播方法\n",
    "将其输入转换为输出\n",
    "并存储任何必要的参数。\n",
    "请注意，有些模块根本不需要任何参数。\n",
    "最后，模块必须具备反向传播方法，\n",
    "用于计算梯度。\n",
    "幸运的是，由于自动微分\n",
    "（在:numref:`sec_autograd`中介绍）\n",
    "提供的幕后魔法，\n",
    "在定义自己的模块时，\n",
    "我们只需要关心参数\n",
    "和前向传播方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b911ed7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:38.169811Z",
     "iopub.status.busy": "2023-08-18T19:31:38.169219Z",
     "iopub.status.idle": "2023-08-18T19:31:40.246403Z",
     "shell.execute_reply": "2023-08-18T19:31:40.245375Z"
    },
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80abfaa",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "[**首先，我们回顾一下用于实现多层感知机的代码**]\n",
    "(:numref:`sec_mlp`)。\n",
    "以下代码生成一个网络，\n",
    "包含一个具有256个单元和ReLU激活函数的全连接隐藏层，\n",
    "接着是一个具有十个单元的全连接输出层（无激活函数）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7df830d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:40.251527Z",
     "iopub.status.busy": "2023-08-18T19:31:40.250671Z",
     "iopub.status.idle": "2023-08-18T19:31:40.284734Z",
     "shell.execute_reply": "2023-08-18T19:31:40.283757Z"
    },
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe16ebaf",
   "metadata": {
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "在这个例子中，我们通过实例化一个`nn.Sequential`来构建我们的模型，按照它们应该被执行的顺序将层作为参数传递。简而言之，（**`nn.Sequential`定义了一种特殊的`Module`**），这是在PyTorch中表示模块的类。它维护了一个组成`Module`的有序列表。请注意，这两个全连接层都是`Linear`类的实例，而`Linear`类本身是`Module`的子类。前向传播（`forward`）方法也非常简单：它将列表中的每个模块串联起来，将每个模块的输出作为下一个模块的输入。请注意，到目前为止，我们一直通过构造`net(X)`来调用我们的模型以获得其输出。这实际上是`net.__call__(X)`的简写。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93754877",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "## [**一个自定义模块**]\n",
    "\n",
    "或许理解模块工作原理最简单的方法\n",
    "就是自己实现一个。\n",
    "在此之前，\n",
    "我们简要总结每个模块必须提供的基本功能：\n",
    "\n",
    "\n",
    "1. 通过其前向传播方法接收输入数据作为参数。\n",
    "1. 通过让前向传播方法返回一个值来生成输出。请注意，输出的形状可能与输入不同。例如，上面模型中的第一个全连接层接收任意维度的输入但返回256维的输出。\n",
    "1. 计算其输出相对于输入的梯度，这可以通过其反向传播方法访问。通常这是自动完成的。\n",
    "1. 存储并提供执行前向传播计算所需的参数。\n",
    "1. 按需初始化模型参数。\n",
    "\n",
    "\n",
    "在下面的代码片段中，\n",
    "我们从零开始编写一个模块，\n",
    "对应一个具有256个隐藏单元的单隐藏层MLP，\n",
    "和一个10维的输出层。\n",
    "请注意，下面的`MLP`类继承了表示模块的类。\n",
    "我们将大量依赖父类的方法，\n",
    "仅提供自己的构造函数（Python中的`__init__`方法）和前向传播方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c5f010e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:40.289115Z",
     "iopub.status.busy": "2023-08-18T19:31:40.288828Z",
     "iopub.status.idle": "2023-08-18T19:31:40.295756Z",
     "shell.execute_reply": "2023-08-18T19:31:40.294461Z"
    },
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Call the constructor of the parent class nn.Module to perform\n",
    "        # the necessary initialization\n",
    "        super().__init__()\n",
    "        self.hidden = nn.LazyLinear(256)\n",
    "        self.out = nn.LazyLinear(10)\n",
    "\n",
    "    # Define the forward propagation of the model, that is, how to return the\n",
    "    # required model output based on the input X\n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7eaced",
   "metadata": {
    "origin_pos": 19
   },
   "source": [
    "让我们首先关注前向传播方法。\n",
    "请注意，它以`X`作为输入，\n",
    "计算应用激活函数后的隐藏表示，\n",
    "并输出其对数。\n",
    "在这个`MLP`实现中，\n",
    "两层都是实例变量。\n",
    "要明白为什么这是合理的，想象一下\n",
    "实例化两个MLP，`net1`和`net2`，\n",
    "并在不同的数据上训练它们。\n",
    "自然而然地，我们期望它们\n",
    "代表两个不同的学习模型。\n",
    "\n",
    "我们在构造函数中[**实例化MLP的层**]\n",
    "（并在每次调用前向传播方法时**调用这些层**）。\n",
    "注意一些关键细节。\n",
    "首先，我们自定义的`__init__`方法\n",
    "通过`super().__init__()`调用了父类的`__init__`方法，\n",
    "省去了重复大多数模块适用的样板代码的麻烦。\n",
    "然后我们实例化了两个全连接层，\n",
    "将它们分配给`self.hidden`和`self.out`。\n",
    "除非我们实现一个新的层，\n",
    "否则我们不必担心反向传播方法\n",
    "或参数初始化。\n",
    "系统会自动生成这些方法。\n",
    "让我们来试试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c8301dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:40.300597Z",
     "iopub.status.busy": "2023-08-18T19:31:40.300120Z",
     "iopub.status.idle": "2023-08-18T19:31:40.308051Z",
     "shell.execute_reply": "2023-08-18T19:31:40.307090Z"
    },
    "origin_pos": 20,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cef5ea",
   "metadata": {
    "origin_pos": 22
   },
   "source": [
    "模块抽象的一个关键优点是其多功能性。\n",
    "我们可以通过子类化模块来创建层\n",
    "（例如全连接层类），\n",
    "整个模型（例如上面的`MLP`类），\n",
    "或者各种中等复杂度的组件。\n",
    "在接下来的章节中，我们将利用这种多功能性，\n",
    "例如在处理卷积神经网络时。\n",
    "\n",
    "\n",
    "## [**Sequential 模块**]\n",
    ":label:`subsec_model-construction-sequential`\n",
    "\n",
    "现在我们可以更仔细地看看\n",
    "`Sequential` 类是如何工作的。\n",
    "回想一下，`Sequential` 被设计为\n",
    "将其他模块串联在一起。\n",
    "要构建我们自己的简化版 `MySequential`，\n",
    "我们只需要定义两个关键方法：\n",
    "\n",
    "1. 一种将模块逐个添加到列表中的方法。\n",
    "1. 一种前向传播方法，用于将输入按添加顺序通过模块链传递。\n",
    "\n",
    "下面的 `MySequential` 类提供了与默认 `Sequential` 类相同的功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09b7f913",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:40.312685Z",
     "iopub.status.busy": "2023-08-18T19:31:40.312400Z",
     "iopub.status.idle": "2023-08-18T19:31:40.318061Z",
     "shell.execute_reply": "2023-08-18T19:31:40.317031Z"
    },
    "origin_pos": 24,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            self.add_module(str(idx), module)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for module in self.children():\n",
    "            X = module(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6a2da4",
   "metadata": {
    "origin_pos": 28,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "在`__init__`方法中，我们通过调用`add_modules`方法来添加每个模块。这些模块可以在以后通过`children`方法访问。这样，系统就能知道所添加的模块，并会正确地初始化每个模块的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008229d2",
   "metadata": {
    "origin_pos": 29
   },
   "source": [
    "当我们调用`MySequential`的前向传播方法时，\n",
    "每个添加的模块都会按照它们被添加的顺序执行。\n",
    "我们现在可以使用我们的`MySequential`类重新实现一个MLP。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b8d5d6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:40.323023Z",
     "iopub.status.busy": "2023-08-18T19:31:40.322454Z",
     "iopub.status.idle": "2023-08-18T19:31:40.330187Z",
     "shell.execute_reply": "2023-08-18T19:31:40.329340Z"
    },
    "origin_pos": 31,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98fc5fe",
   "metadata": {
    "origin_pos": 34
   },
   "source": [
    "请注意，这里使用`MySequential`\n",
    "与我们之前为`Sequential`类编写的代码\n",
    "（如在:numref:`sec_mlp`中所述）是完全相同的。\n",
    "\n",
    "\n",
    "## [**在前向传播方法中执行代码**]\n",
    "\n",
    "`Sequential`类使模型构建变得简单，\n",
    "允许我们在不需要定义自己的类的情况下\n",
    "组装新的架构。\n",
    "然而，并非所有架构都是简单的链式结构。\n",
    "当需要更大的灵活性时，\n",
    "我们将希望定义自己的模块。\n",
    "例如，我们可能希望在前向传播方法中\n",
    "执行Python的控制流。\n",
    "此外，我们可能希望执行\n",
    "任意数学运算，\n",
    "而不仅仅是依赖预定义的神经网络层。\n",
    "\n",
    "你可能已经注意到，到目前为止，\n",
    "我们网络中的所有操作\n",
    "都作用于网络的激活和参数。\n",
    "然而，有时我们可能希望\n",
    "包含一些既不是先前层的结果\n",
    "也不是可更新参数的项。\n",
    "我们称这些为*常数参数*。\n",
    "比如说，我们希望有一个层\n",
    "计算函数\n",
    "$f(\\mathbf{x},\\mathbf{w}) = c \\cdot \\mathbf{w}^\\top \\mathbf{x}$，\n",
    "其中$\\mathbf{x}$是输入，$\\mathbf{w}$是我们的参数，\n",
    "而$c$是某个指定的常数，\n",
    "它在优化过程中不会被更新。\n",
    "因此，我们实现了一个`FixedHiddenMLP`类如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f8721f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:40.334075Z",
     "iopub.status.busy": "2023-08-18T19:31:40.333497Z",
     "iopub.status.idle": "2023-08-18T19:31:40.340281Z",
     "shell.execute_reply": "2023-08-18T19:31:40.339397Z"
    },
    "origin_pos": 36,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Random weight parameters that will not compute gradients and\n",
    "        # therefore keep constant during training\n",
    "        self.rand_weight = torch.rand((20, 20))\n",
    "        self.linear = nn.LazyLinear(20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(X @ self.rand_weight + 1)\n",
    "        # Reuse the fully connected layer. This is equivalent to sharing\n",
    "        # parameters with two fully connected layers\n",
    "        X = self.linear(X)\n",
    "        # Control flow\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e65b0b",
   "metadata": {
    "origin_pos": 39
   },
   "source": [
    "在这个模型中，\n",
    "我们实现了一个隐藏层，其权重\n",
    "（`self.rand_weight`）在实例化时随机初始化，\n",
    "之后保持不变。\n",
    "这个权重不是模型参数，\n",
    "因此它永远不会通过反向传播更新。\n",
    "然后网络将这个“固定”层的输出\n",
    "传递给一个全连接层。\n",
    "\n",
    "需要注意的是，在返回输出之前，\n",
    "我们的模型做了一些不寻常的事情。\n",
    "我们运行了一个 while 循环，测试\n",
    "其 $\\ell_1$ 范数是否大于 $1$，\n",
    "并将输出向量除以 $2$，\n",
    "直到满足条件为止。\n",
    "最后，我们返回了 `X` 中元素的总和。\n",
    "据我们所知，没有标准的神经网络\n",
    "执行此操作。\n",
    "请注意，这种特定的操作可能对\n",
    "任何实际任务都没有用处。\n",
    "我们的目的只是向您展示如何将\n",
    "任意代码集成到神经网络计算流程中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ede5347f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:40.344398Z",
     "iopub.status.busy": "2023-08-18T19:31:40.343674Z",
     "iopub.status.idle": "2023-08-18T19:31:40.355810Z",
     "shell.execute_reply": "2023-08-18T19:31:40.353856Z"
    },
    "origin_pos": 40,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.3836, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e343c1a3",
   "metadata": {
    "origin_pos": 42
   },
   "source": [
    "我们可以[**混合搭配各种模块组装方式。**]\n",
    "在下面的例子中，我们以一些创造性的方式嵌套模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0c3d190",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:40.359588Z",
     "iopub.status.busy": "2023-08-18T19:31:40.359258Z",
     "iopub.status.idle": "2023-08-18T19:31:40.372492Z",
     "shell.execute_reply": "2023-08-18T19:31:40.371497Z"
    },
    "origin_pos": 44,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0679, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.LazyLinear(64), nn.ReLU(),\n",
    "                                 nn.LazyLinear(32), nn.ReLU())\n",
    "        self.linear = nn.LazyLinear(16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.LazyLinear(20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48005bbf",
   "metadata": {
    "origin_pos": 47
   },
   "source": [
    "## 摘要\n",
    "\n",
    "单个层可以是模块。\n",
    "许多层可以组成一个模块。\n",
    "许多模块可以组成一个模块。\n",
    "\n",
    "模块可以包含代码。\n",
    "模块处理许多杂务，包括参数初始化和反向传播。\n",
    "层和模块的顺序连接由`Sequential`模块处理。\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 如果你将`MySequential`改为在Python列表中存储模块，会出现什么问题？\n",
    "1. 实现一个模块，该模块接受两个模块作为参数，例如`net1`和`net2`，并在前向传播中返回这两个网络的连接输出。这也称为*并行模块*。\n",
    "1. 假设你想连接多个相同的网络实例。实现一个工厂函数，生成同一模块的多个实例，并从中构建一个更大的网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3116594",
   "metadata": {
    "origin_pos": 49,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[讨论](https://discuss.d2l.ai/t/55)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}