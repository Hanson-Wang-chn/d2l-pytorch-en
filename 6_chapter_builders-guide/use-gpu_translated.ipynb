{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25fee3b7",
   "metadata": {
    "origin_pos": 1
   },
   "source": [
    "# GPU\n",
    ":label:`sec_use_gpu`\n",
    "\n",
    "在:numref:`tab_intro_decade`中，我们展示了过去二十年计算能力的快速增长。简而言之，自2000年以来，GPU性能每十年增长1000倍。这提供了巨大的机会，但也表明了对这种性能有显著的需求。\n",
    "\n",
    "在本节中，我们将开始讨论如何利用这种计算性能来进行你的研究。首先使用单个GPU，之后再讨论如何使用多个GPU和多台服务器（配备多个GPU）。\n",
    "\n",
    "具体来说，我们将讨论如何使用单个NVIDIA GPU进行计算。首先，确保至少安装了一个NVIDIA GPU。然后，下载[NVIDIA驱动程序和CUDA](https://developer.nvidia.com/cuda-downloads)，并按照提示设置适当的路径。一旦这些准备工作完成，可以使用`nvidia-smi`命令来（**查看显卡信息**）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92058ee6",
   "metadata": {
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "在PyTorch中，每个数组都有一个设备；我们经常将其称为*上下文*。到目前为止，默认情况下，所有变量及其相关计算都被分配到了CPU上。通常，其他上下文可能是各种GPU。当我们跨多台服务器部署任务时，事情可能会变得更加复杂。通过智能地将数组分配给上下文，我们可以最小化设备之间传输数据所花费的时间。例如，在具有GPU的服务器上训练神经网络时，我们通常希望模型的参数位于GPU上。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bda574",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "要运行本节中的程序，\n",
    "您至少需要两个GPU。\n",
    "请注意，这对于大多数台式计算机来说可能过于奢侈，\n",
    "但在云中很容易获得，例如，\n",
    "通过使用AWS EC2多GPU实例。\n",
    "几乎所有其他部分都*不需要*多个GPU，但在这里我们只是想说明不同设备之间的数据流。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4ef6e6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:36:58.270913Z",
     "iopub.status.busy": "2023-08-18T19:36:58.270055Z",
     "iopub.status.idle": "2023-08-18T19:37:01.897059Z",
     "shell.execute_reply": "2023-08-18T19:37:01.896067Z"
    },
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0006dfe3",
   "metadata": {
    "origin_pos": 9
   },
   "source": [
    "## [**计算设备**]\n",
    "\n",
    "我们可以指定诸如CPU和GPU之类的设备，\n",
    "用于存储和计算。\n",
    "默认情况下，张量在主内存中创建，\n",
    "然后使用CPU进行计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff8c64e",
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "在 PyTorch 中，CPU 和 GPU 可以用 `torch.device('cpu')` 和 `torch.device('cuda')` 表示。\n",
    "需要注意的是，`cpu` 设备\n",
    "意味着所有物理 CPU 和内存。\n",
    "这意味着 PyTorch 的计算\n",
    "将尝试使用所有 CPU 核心。\n",
    "然而，`gpu` 设备仅表示一张卡\n",
    "及其对应的内存。\n",
    "如果有多个 GPU，我们使用 `torch.device(f'cuda:{i}')`\n",
    "来表示第 $i$ 个 GPU（$i$ 从 0 开始）。\n",
    "另外，`gpu:0` 和 `gpu` 是等价的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d996a07b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:37:01.901957Z",
     "iopub.status.busy": "2023-08-18T19:37:01.901006Z",
     "iopub.status.idle": "2023-08-18T19:37:01.911076Z",
     "shell.execute_reply": "2023-08-18T19:37:01.909836Z"
    },
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'),\n",
       " device(type='cuda', index=0),\n",
       " device(type='cuda', index=1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cpu():  #@save\n",
    "    \"\"\"Get the CPU device.\"\"\"\n",
    "    return torch.device('cpu')\n",
    "\n",
    "def gpu(i=0):  #@save\n",
    "    \"\"\"Get a GPU device.\"\"\"\n",
    "    return torch.device(f'cuda:{i}')\n",
    "\n",
    "cpu(), gpu(), gpu(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a643379",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "我们可以（查询可用的GPU数量。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b20d4266",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:37:01.915209Z",
     "iopub.status.busy": "2023-08-18T19:37:01.914386Z",
     "iopub.status.idle": "2023-08-18T19:37:01.922363Z",
     "shell.execute_reply": "2023-08-18T19:37:01.921100Z"
    },
    "origin_pos": 15,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def num_gpus():  #@save\n",
    "    \"\"\"Get the number of available GPUs.\"\"\"\n",
    "    return torch.cuda.device_count()\n",
    "\n",
    "num_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab10cfc5",
   "metadata": {
    "origin_pos": 17
   },
   "source": [
    "现在我们[**定义两个方便的函数，即使请求的GPU不存在，也能让我们运行代码。**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ac547f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:37:01.926431Z",
     "iopub.status.busy": "2023-08-18T19:37:01.925574Z",
     "iopub.status.idle": "2023-08-18T19:37:01.935019Z",
     "shell.execute_reply": "2023-08-18T19:37:01.933960Z"
    },
    "origin_pos": 18,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0),\n",
       " device(type='cpu'),\n",
       " [device(type='cuda', index=0), device(type='cuda', index=1)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def try_gpu(i=0):  #@save\n",
    "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
    "    if num_gpus() >= i + 1:\n",
    "        return gpu(i)\n",
    "    return cpu()\n",
    "\n",
    "def try_all_gpus():  #@save\n",
    "    \"\"\"Return all available GPUs, or [cpu(),] if no GPU exists.\"\"\"\n",
    "    return [gpu(i) for i in range(num_gpus())]\n",
    "\n",
    "try_gpu(), try_gpu(10), try_all_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d23836",
   "metadata": {
    "origin_pos": 19
   },
   "source": [
    "## 张量和GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04367f7",
   "metadata": {
    "origin_pos": 20,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "默认情况下，张量是在CPU上创建的。\n",
    "我们可以[**查询张量所在的设备。**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3e90ced",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:37:01.939959Z",
     "iopub.status.busy": "2023-08-18T19:37:01.938949Z",
     "iopub.status.idle": "2023-08-18T19:37:01.950067Z",
     "shell.execute_reply": "2023-08-18T19:37:01.949195Z"
    },
    "origin_pos": 24,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "x.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c538315e",
   "metadata": {
    "origin_pos": 27
   },
   "source": [
    "需要注意的是，每当我们要对多个项进行操作时，它们需要位于同一设备上。例如，如果我们对两个张量求和，我们需要确保两个参数都位于同一设备上——否则框架将不知道在哪里存储结果，甚至不知道如何决定在哪里执行计算。\n",
    "\n",
    "### GPU上的存储\n",
    "\n",
    "有几种方法可以[**将张量存储在GPU上**]。例如，我们可以在创建张量时指定存储设备。接下来，我们在第一个`gpu`上创建张量变量`X`。在GPU上创建的张量只消耗该GPU的内存。我们可以使用`nvidia-smi`命令来查看GPU的内存使用情况。通常，我们需要确保不会创建超出GPU内存限制的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13913886",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:37:01.953772Z",
     "iopub.status.busy": "2023-08-18T19:37:01.953191Z",
     "iopub.status.idle": "2023-08-18T19:37:02.420258Z",
     "shell.execute_reply": "2023-08-18T19:37:02.419290Z"
    },
    "origin_pos": 29,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones(2, 3, device=try_gpu())\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ea65fc",
   "metadata": {
    "origin_pos": 32
   },
   "source": [
    "假设你至少有两个GPU，以下代码将（**在第二个GPU上创建一个随机张量 `Y`。**）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f4c7aff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:37:02.424924Z",
     "iopub.status.busy": "2023-08-18T19:37:02.424008Z",
     "iopub.status.idle": "2023-08-18T19:37:02.688334Z",
     "shell.execute_reply": "2023-08-18T19:37:02.687371Z"
    },
    "origin_pos": 34,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0022, 0.5723, 0.2890],\n",
       "        [0.1456, 0.3537, 0.7359]], device='cuda:1')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.rand(2, 3, device=try_gpu(1))\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bf740b",
   "metadata": {
    "origin_pos": 37
   },
   "source": [
    "### 复制\n",
    "\n",
    "[**如果我们想要计算 `X + Y`，\n",
    "我们需要决定在哪里执行这个操作。**]\n",
    "例如，如 :numref:`fig_copyto` 所示，\n",
    "我们可以将 `X` 转移到第二个 GPU\n",
    "并在那里执行操作。\n",
    "*不要*简单地将 `X` 和 `Y` 相加，\n",
    "因为这会导致异常。\n",
    "运行时引擎将不知道如何处理：\n",
    "它无法在同一设备上找到数据，因此会失败。\n",
    "由于 `Y` 存储在第二个 GPU 上，\n",
    "我们需要先将 `X` 移动到那里才能进行相加。\n",
    "\n",
    "![Copy data to perform an operation on the same device.](../img/copyto.svg)\n",
    ":label:`fig_copyto`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3560f0b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:37:02.693634Z",
     "iopub.status.busy": "2023-08-18T19:37:02.693201Z",
     "iopub.status.idle": "2023-08-18T19:37:02.701839Z",
     "shell.execute_reply": "2023-08-18T19:37:02.701004Z"
    },
    "origin_pos": 39,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], device='cuda:0')\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "Z = X.cuda(1)\n",
    "print(X)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc2252c",
   "metadata": {
    "origin_pos": 42
   },
   "source": [
    "现在[**数据（包括`Z`和`Y`）都在同一GPU上，我们可以将它们相加。**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cfea6e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:37:02.707070Z",
     "iopub.status.busy": "2023-08-18T19:37:02.705679Z",
     "iopub.status.idle": "2023-08-18T19:37:02.735588Z",
     "shell.execute_reply": "2023-08-18T19:37:02.734193Z"
    },
    "origin_pos": 43,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0022, 1.5723, 1.2890],\n",
       "        [1.1456, 1.3537, 1.7359]], device='cuda:1')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y + Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4657c339",
   "metadata": {
    "origin_pos": 45,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "但是如果变量`Z`已经存在于你的第二个GPU上呢？\n",
    "如果我们仍然调用`Z.cuda(1)`会发生什么？\n",
    "它会返回`Z`，而不会创建副本或分配新的内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0450cb7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:37:02.743585Z",
     "iopub.status.busy": "2023-08-18T19:37:02.743275Z",
     "iopub.status.idle": "2023-08-18T19:37:02.750645Z",
     "shell.execute_reply": "2023-08-18T19:37:02.748215Z"
    },
    "origin_pos": 49,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z.cuda(1) is Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5658048c",
   "metadata": {
    "origin_pos": 52
   },
   "source": [
    "### 旁注\n",
    "\n",
    "人们使用GPU进行机器学习\n",
    "是因为他们期望它们速度快。\n",
    "但是在设备之间传输变量很慢：比计算慢得多。\n",
    "所以我们希望你100%确定\n",
    "在我们允许你这样做之前，你确实想要执行一个缓慢的操作。\n",
    "如果深度学习框架只是自动进行了复制\n",
    "而没有崩溃，那么你可能不会意识到\n",
    "你写了一些效率低下的代码。\n",
    "\n",
    "数据传输不仅慢，还使得并行化变得更加困难，\n",
    "因为我们必须等待数据被发送（或者更准确地说是被接收）\n",
    "才能继续进行更多的操作。\n",
    "这就是为什么复制操作需要非常小心。\n",
    "作为经验法则，许多小操作\n",
    "比一个大操作糟糕得多。\n",
    "此外，同时进行多个操作\n",
    "比在代码中分散的多个单个操作要好得多，\n",
    "除非你知道自己在做什么。\n",
    "这是因为这种操作可能会阻塞，如果一个设备\n",
    "必须等待另一个设备完成操作后才能做其他事情。\n",
    "这有点像排队点咖啡\n",
    "而不是通过电话预先下单\n",
    "然后发现当你到达时咖啡已经准备好了。\n",
    "\n",
    "最后，当我们打印张量或将张量转换为NumPy格式时，\n",
    "如果数据不在主内存中，\n",
    "框架会先将其复制到主内存中，\n",
    "从而导致额外的传输开销。\n",
    "更糟的是，它现在受制于可怕的全局解释器锁\n",
    "这使得所有事情都得等待Python完成。\n",
    "\n",
    "\n",
    "## [**神经网络和GPU**]\n",
    "\n",
    "类似地，神经网络模型可以指定设备。\n",
    "以下代码将模型参数放在GPU上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bcc281a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:37:02.756785Z",
     "iopub.status.busy": "2023-08-18T19:37:02.756022Z",
     "iopub.status.idle": "2023-08-18T19:37:02.763247Z",
     "shell.execute_reply": "2023-08-18T19:37:02.762013Z"
    },
    "origin_pos": 54,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(1))\n",
    "net = net.to(device=try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb2c254",
   "metadata": {
    "origin_pos": 57
   },
   "source": [
    "在接下来的章节中，我们将看到更多关于如何在GPU上运行模型的例子，仅仅因为这些模型的计算量会稍微增加。\n",
    "\n",
    "例如，当输入是GPU上的张量时，模型将在同一个GPU上计算结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "351af69d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:37:02.768539Z",
     "iopub.status.busy": "2023-08-18T19:37:02.767413Z",
     "iopub.status.idle": "2023-08-18T19:37:02.809950Z",
     "shell.execute_reply": "2023-08-18T19:37:02.807298Z"
    },
    "origin_pos": 58,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7802],\n",
       "        [0.7802]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad9b55b",
   "metadata": {
    "origin_pos": 60
   },
   "source": [
    "让我们（**确认模型参数存储在同一GPU上。**）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fdbd2c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:37:02.816317Z",
     "iopub.status.busy": "2023-08-18T19:37:02.815749Z",
     "iopub.status.idle": "2023-08-18T19:37:02.822467Z",
     "shell.execute_reply": "2023-08-18T19:37:02.821657Z"
    },
    "origin_pos": 62,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5940ac",
   "metadata": {
    "origin_pos": 65
   },
   "source": [
    "让训练器支持GPU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1283ae3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:37:02.826029Z",
     "iopub.status.busy": "2023-08-18T19:37:02.825482Z",
     "iopub.status.idle": "2023-08-18T19:37:02.832065Z",
     "shell.execute_reply": "2023-08-18T19:37:02.831156Z"
    },
    "origin_pos": 67,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.Trainer)  #@save\n",
    "def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\n",
    "    self.save_hyperparameters()\n",
    "    self.gpus = [d2l.gpu(i) for i in range(min(num_gpus, d2l.num_gpus()))]\n",
    "\n",
    "@d2l.add_to_class(d2l.Trainer)  #@save\n",
    "def prepare_batch(self, batch):\n",
    "    if self.gpus:\n",
    "        batch = [a.to(self.gpus[0]) for a in batch]\n",
    "    return batch\n",
    "\n",
    "@d2l.add_to_class(d2l.Trainer)  #@save\n",
    "def prepare_model(self, model):\n",
    "    model.trainer = self\n",
    "    model.board.xlim = [0, self.max_epochs]\n",
    "    if self.gpus:\n",
    "        model.to(self.gpus[0])\n",
    "    self.model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f33c768",
   "metadata": {
    "origin_pos": 69
   },
   "source": [
    "简而言之，只要所有数据和参数都在同一设备上，我们就可以高效地学习模型。在接下来的章节中，我们将看到几个这样的例子。\n",
    "\n",
    "## 总结\n",
    "\n",
    "我们可以指定用于存储和计算的设备，例如CPU或GPU。\n",
    "  默认情况下，数据是在主内存中创建\n",
    "  然后使用CPU进行计算。\n",
    "深度学习框架要求所有用于计算的输入数据\n",
    "  都必须在同一设备上，\n",
    "  无论是CPU还是同一个GPU。\n",
    "不加注意地移动数据可能会导致性能显著下降。\n",
    "  一个典型的错误如下：在GPU上为每个小批量计算损失\n",
    "  并通过命令行（或记录到NumPy `ndarray`中）将其报告给用户\n",
    "  这将触发全局解释器锁，从而阻塞所有GPU。\n",
    "  更好的做法是分配\n",
    "  GPU内的日志内存，并仅移动较大的日志。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 尝试更大的计算任务，比如大矩阵的乘法，\n",
    "   并观察CPU和GPU之间的速度差异。\n",
    "   如果是一个计算量很小的任务呢？\n",
    "1. 我们应该如何在GPU上读写模型参数？\n",
    "1. 测量计算1000次\n",
    "   $100 \\times 100$矩阵-矩阵乘法所需的时间\n",
    "   并每次记录输出矩阵的Frobenius范数。与在GPU上保持日志并将最终结果转移进行比较。\n",
    "1. 测量同时在两个GPU上执行两次矩阵-矩阵乘法所需的时间。\n",
    "   与在一个GPU上顺序计算进行比较。提示：你应该能看到几乎线性的扩展。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cfc42b",
   "metadata": {
    "origin_pos": 71,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[讨论](https://discuss.d2l.ai/t/63)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}