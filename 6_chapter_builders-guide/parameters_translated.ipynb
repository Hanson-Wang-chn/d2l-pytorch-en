{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7443df46",
   "metadata": {
    "origin_pos": 1
   },
   "source": [
    "# 参数管理\n",
    "\n",
    "一旦我们选择了架构并设置了超参数，我们就进入训练循环，在这个循环中我们的目标是找到使损失函数最小化的参数值。训练完成后，为了进行未来的预测，我们需要这些参数。此外，有时我们希望提取参数，可能是为了在其他上下文中重用它们，将模型保存到磁盘以便在其他软件中执行，或者为了通过检查来获得科学理解。\n",
    "\n",
    "大多数时候，我们可以忽略参数声明和操作的具体细节，依靠深度学习框架来完成繁重的工作。然而，当我们远离具有标准层的堆叠架构时，有时需要深入了解参数的声明和操作。在本节中，我们将涵盖以下内容：\n",
    "\n",
    "* 为调试、诊断和可视化访问参数。\n",
    "* 在不同的模型组件之间共享参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41cbf7e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:27:20.807089Z",
     "iopub.status.busy": "2023-08-18T19:27:20.806426Z",
     "iopub.status.idle": "2023-08-18T19:27:22.457089Z",
     "shell.execute_reply": "2023-08-18T19:27:22.456154Z"
    },
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293084ba",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "（**我们首先关注一个含有一层隐藏层的多层感知机。**）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aa0461f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:27:22.461279Z",
     "iopub.status.busy": "2023-08-18T19:27:22.460607Z",
     "iopub.status.idle": "2023-08-18T19:27:22.494399Z",
     "shell.execute_reply": "2023-08-18T19:27:22.493545Z"
    },
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(8),\n",
    "                    nn.ReLU(),\n",
    "                    nn.LazyLinear(1))\n",
    "\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0bb094",
   "metadata": {
    "origin_pos": 11
   },
   "source": [
    "## [**参数访问**]\n",
    ":label:`subsec_param-access`\n",
    "\n",
    "让我们从如何访问你已知模型中的参数开始。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e03323",
   "metadata": {
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "当通过`Sequential`类定义模型时，我们可以通过像列表一样索引模型来访问任何层。每个层的参数都方便地存储在其属性中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f41ac20",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "我们可以检查第二个全连接层的参数如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c6fdb60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:27:22.497996Z",
     "iopub.status.busy": "2023-08-18T19:27:22.497442Z",
     "iopub.status.idle": "2023-08-18T19:27:22.504291Z",
     "shell.execute_reply": "2023-08-18T19:27:22.503521Z"
    },
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ 0.1097, -0.2902, -0.2685,  0.0790, -0.1595,  0.0364,  0.2286,  0.0852]])),\n",
       "             ('bias', tensor([-0.2319]))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9e6e49",
   "metadata": {
    "origin_pos": 19
   },
   "source": [
    "我们可以看到这个全连接层包含两个参数，分别对应该层的权重和偏置。\n",
    "\n",
    "### [**目标参数**]\n",
    "\n",
    "请注意，每个参数都表示为参数类的一个实例。要对这些参数进行任何有用的操作，我们首先需要访问其底层的数值。有几种方法可以做到这一点，有些方法更简单，而有些则更为通用。以下代码从第二神经网络层中提取偏置，这将返回一个参数类实例，并进一步访问该参数的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba2da7b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:27:22.507849Z",
     "iopub.status.busy": "2023-08-18T19:27:22.507181Z",
     "iopub.status.idle": "2023-08-18T19:27:22.513236Z",
     "shell.execute_reply": "2023-08-18T19:27:22.512406Z"
    },
    "origin_pos": 21,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.nn.parameter.Parameter, tensor([-0.2319]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(net[2].bias), net[2].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c10cf6c",
   "metadata": {
    "origin_pos": 24,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "参数是复杂的对象，\n",
    "包含值、梯度\n",
    "以及额外的信息。\n",
    "这就是为什么我们需要显式地请求值。\n",
    "\n",
    "除了值之外，每个参数还允许我们访问梯度。因为我们还没有为这个网络调用反向传播，所以它处于初始状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c5f0ae9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:27:22.516723Z",
     "iopub.status.busy": "2023-08-18T19:27:22.516170Z",
     "iopub.status.idle": "2023-08-18T19:27:22.521606Z",
     "shell.execute_reply": "2023-08-18T19:27:22.520790Z"
    },
    "origin_pos": 27,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.grad == None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d744bc",
   "metadata": {
    "origin_pos": 28
   },
   "source": [
    "### [**一次性处理所有参数**]\n",
    "\n",
    "当我们需要对所有参数执行操作时，\n",
    "逐一访问它们可能会变得很繁琐。\n",
    "特别是在处理更复杂的情况，例如嵌套模块时，\n",
    "情况会变得更加难以管理，\n",
    "因为我们需要递归遍历整个树来提取\n",
    "每个子模块的参数。下面我们将演示如何访问所有层的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dab1b4b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:27:22.525019Z",
     "iopub.status.busy": "2023-08-18T19:27:22.524380Z",
     "iopub.status.idle": "2023-08-18T19:27:22.530002Z",
     "shell.execute_reply": "2023-08-18T19:27:22.529195Z"
    },
    "origin_pos": 30,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0.weight', torch.Size([8, 4])),\n",
       " ('0.bias', torch.Size([8])),\n",
       " ('2.weight', torch.Size([1, 8])),\n",
       " ('2.bias', torch.Size([1]))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(name, param.shape) for name, param in net.named_parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd29a2e",
   "metadata": {
    "origin_pos": 33
   },
   "source": [
    "## [**Tied Parameters**]\n",
    "\n",
    "通常，我们希望在多个层之间共享参数。\n",
    "让我们看看如何优雅地实现这一点。\n",
    "在下面的例子中，我们分配了一个全连接层，\n",
    "然后专门使用它的参数来设置另一个层的参数。\n",
    "这里我们需要运行前向传播\n",
    "`net(X)` 才能访问参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b706636",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:27:22.533421Z",
     "iopub.status.busy": "2023-08-18T19:27:22.532786Z",
     "iopub.status.idle": "2023-08-18T19:27:22.541856Z",
     "shell.execute_reply": "2023-08-18T19:27:22.541011Z"
    },
    "origin_pos": 35,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# We need to give the shared layer a name so that we can refer to its\n",
    "# parameters\n",
    "shared = nn.LazyLinear(8)\n",
    "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.LazyLinear(1))\n",
    "\n",
    "net(X)\n",
    "# Check whether the parameters are the same\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "# Make sure that they are actually the same object rather than just having the\n",
    "# same value\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec93f84",
   "metadata": {
    "origin_pos": 38
   },
   "source": [
    "这个例子表明第二层和第三层的参数是绑定的。它们不仅仅是相等，而是由完全相同的张量表示。因此，如果我们更改其中一个参数，另一个也会随之改变。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7b800f",
   "metadata": {
    "origin_pos": 39,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "你可能想知道，\n",
    "当参数被绑定时\n",
    "梯度会发生什么？\n",
    "由于模型参数包含梯度，\n",
    "在反向传播过程中\n",
    "第二隐藏层和第三隐藏层的梯度会被加在一起。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f0f0ad",
   "metadata": {
    "origin_pos": 40
   },
   "source": [
    "## 摘要\n",
    "\n",
    "我们有多种方法可以访问和绑定模型参数。\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 使用在 :numref:`sec_model_construction` 中定义的 `NestMLP` 模型，访问各层的参数。\n",
    "1. 构建一个包含共享参数层的多层感知机并进行训练。在训练过程中，观察每一层的模型参数和梯度。\n",
    "1. 为什么共享参数是一个好主意？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3987814f",
   "metadata": {
    "origin_pos": 42,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[讨论](https://discuss.d2l.ai/t/57)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-en",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
