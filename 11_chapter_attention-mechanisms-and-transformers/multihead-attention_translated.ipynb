{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e0727ae",
   "metadata": {
    "origin_pos": 1
   },
   "source": [
    "# 多头注意力\n",
    ":label:`sec_multihead-attention`\n",
    "\n",
    "在实践中，给定相同的查询、键和值集合，我们可能希望我们的模型能够结合同一注意力机制的不同行为的知识，例如捕捉序列中不同范围的依赖关系（例如，短距离与长距离）。因此，允许我们的注意力机制联合使用查询、键和值的不同表示子空间可能是有益的。\n",
    "\n",
    "为此，不是执行单一的注意力池化，而是可以使用$h$个独立学习的线性投影对查询、键和值进行变换。然后将这些$h$个投影后的查询、键和值并行地输入到注意力池化中。最后，$h$个注意力池化的输出被拼接起来，并通过另一个学习的线性投影转换以产生最终输出。这种设计被称为*多头注意力*，其中每个$h$注意力池化输出是一个*头* :cite:`Vaswani.Shazeer.Parmar.ea.2017`。使用全连接层执行可学习的线性变换，:numref:`fig_multi-head-attention` 描述了多头注意力。\n",
    "\n",
    "![Multi-head attention, where multiple heads are concatenated then linearly transformed.](../img/multi-head-attention.svg)\n",
    ":label:`fig_multi-head-attention`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cf184a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:42:05.720112Z",
     "iopub.status.busy": "2023-08-18T19:42:05.719157Z",
     "iopub.status.idle": "2023-08-18T19:42:08.696163Z",
     "shell.execute_reply": "2023-08-18T19:42:08.694248Z"
    },
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683faf27",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "## 模型\n",
    "\n",
    "在提供多头注意力的实现之前，让我们先从数学上形式化这个模型。给定一个查询 $\\mathbf{q} \\in \\mathbb{R}^{d_q}$、一个键 $\\mathbf{k} \\in \\mathbb{R}^{d_k}$ 和一个值 $\\mathbf{v} \\in \\mathbb{R}^{d_v}$，每个注意力头 $\\mathbf{h}_i$  ($i = 1, \\ldots, h$) 计算如下：\n",
    "\n",
    "$$\\mathbf{h}_i = f(\\mathbf W_i^{(q)}\\mathbf q, \\mathbf W_i^{(k)}\\mathbf k,\\mathbf W_i^{(v)}\\mathbf v) \\in \\mathbb R^{p_v},$$\n",
    "\n",
    "其中\n",
    "$\\mathbf W_i^{(q)}\\in\\mathbb R^{p_q\\times d_q}$,\n",
    "$\\mathbf W_i^{(k)}\\in\\mathbb R^{p_k\\times d_k}$,\n",
    "和 $\\mathbf W_i^{(v)}\\in\\mathbb R^{p_v\\times d_v}$\n",
    "是可学习参数，而\n",
    "$f$ 是注意力池化，\n",
    "例如\n",
    "加性注意力和缩放点积注意力\n",
    "在 :numref:`sec_attention-scoring-functions` 中。\n",
    "多头注意力的输出\n",
    "是通过\n",
    "可学习参数\n",
    "$\\mathbf W_o\\in\\mathbb R^{p_o\\times h p_v}$\n",
    "对 $h$ 个头的拼接进行的另一个线性变换：\n",
    "\n",
    "$$\\mathbf W_o \\begin{bmatrix}\\mathbf h_1\\\\\\vdots\\\\\\mathbf h_h\\end{bmatrix} \\in \\mathbb{R}^{p_o}.$$\n",
    "\n",
    "基于这种设计，每个头可以关注输入的不同部分。\n",
    "比简单的加权平均更复杂的函数可以被表达。\n",
    "\n",
    "## 实现\n",
    "\n",
    "在我们的实现中，\n",
    "我们[**为多头注意力的每个头选择缩放点积注意力**]。\n",
    "为了避免计算成本和参数化成本显著增长，我们设置 $p_q = p_k = p_v = p_o / h$。\n",
    "注意，如果我们将线性变换\n",
    "对于查询、键和值的输出数量\n",
    "设为 $p_q h = p_k h = p_v h = p_o$，则 $h$ 个头可以并行计算。\n",
    "在下面的实现中，\n",
    "$p_o$ 通过参数 `num_hiddens` 指定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdc6ec21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:42:08.702133Z",
     "iopub.status.busy": "2023-08-18T19:42:08.701108Z",
     "iopub.status.idle": "2023-08-18T19:42:08.710324Z",
     "shell.execute_reply": "2023-08-18T19:42:08.709226Z"
    },
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(d2l.Module):  #@save\n",
    "    \"\"\"Multi-head attention.\"\"\"\n",
    "    def __init__(self, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = d2l.DotProductAttention(dropout)\n",
    "        self.W_q = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        self.W_k = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        self.W_v = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        self.W_o = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        # Shape of queries, keys, or values:\n",
    "        # (batch_size, no. of queries or key-value pairs, num_hiddens)\n",
    "        # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n",
    "        # After transposing, shape of output queries, keys, or values:\n",
    "        # (batch_size * num_heads, no. of queries or key-value pairs,\n",
    "        # num_hiddens / num_heads)\n",
    "        queries = self.transpose_qkv(self.W_q(queries))\n",
    "        keys = self.transpose_qkv(self.W_k(keys))\n",
    "        values = self.transpose_qkv(self.W_v(values))\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            # On axis 0, copy the first item (scalar or vector) for num_heads\n",
    "            # times, then copy the next item, and so on\n",
    "            valid_lens = torch.repeat_interleave(\n",
    "                valid_lens, repeats=self.num_heads, dim=0)\n",
    "\n",
    "        # Shape of output: (batch_size * num_heads, no. of queries,\n",
    "        # num_hiddens / num_heads)\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "        # Shape of output_concat: (batch_size, no. of queries, num_hiddens)\n",
    "        output_concat = self.transpose_output(output)\n",
    "        return self.W_o(output_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27562ce8",
   "metadata": {
    "origin_pos": 11
   },
   "source": [
    "为了实现[**多个头的并行计算**]，上述`MultiHeadAttention`类使用了如下定义的两种转置方法。具体来说，`transpose_output`方法执行的是`transpose_qkv`方法操作的逆过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4da9ac2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:42:08.714864Z",
     "iopub.status.busy": "2023-08-18T19:42:08.714207Z",
     "iopub.status.idle": "2023-08-18T19:42:08.723031Z",
     "shell.execute_reply": "2023-08-18T19:42:08.722024Z"
    },
    "origin_pos": 13,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "@d2l.add_to_class(MultiHeadAttention)  #@save\n",
    "def transpose_qkv(self, X):\n",
    "    \"\"\"Transposition for parallel computation of multiple attention heads.\"\"\"\n",
    "    # Shape of input X: (batch_size, no. of queries or key-value pairs,\n",
    "    # num_hiddens). Shape of output X: (batch_size, no. of queries or\n",
    "    # key-value pairs, num_heads, num_hiddens / num_heads)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)\n",
    "    # Shape of output X: (batch_size, num_heads, no. of queries or key-value\n",
    "    # pairs, num_hiddens / num_heads)\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    # Shape of output: (batch_size * num_heads, no. of queries or key-value\n",
    "    # pairs, num_hiddens / num_heads)\n",
    "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "@d2l.add_to_class(MultiHeadAttention)  #@save\n",
    "def transpose_output(self, X):\n",
    "    \"\"\"Reverse the operation of transpose_qkv.\"\"\"\n",
    "    X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(X.shape[0], X.shape[1], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ceb26d",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "让我们用一个简单的例子来[**测试我们实现的**] `MultiHeadAttention` 类，其中键和值是相同的。因此，多头注意力输出的形状是（`batch_size`，`num_queries`，`num_hiddens`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18451bc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:42:08.726610Z",
     "iopub.status.busy": "2023-08-18T19:42:08.726042Z",
     "iopub.status.idle": "2023-08-18T19:42:08.759713Z",
     "shell.execute_reply": "2023-08-18T19:42:08.758787Z"
    },
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "num_hiddens, num_heads = 100, 5\n",
    "attention = MultiHeadAttention(num_hiddens, num_heads, 0.5)\n",
    "batch_size, num_queries, num_kvpairs = 2, 4, 6\n",
    "valid_lens = torch.tensor([3, 2])\n",
    "X = torch.ones((batch_size, num_queries, num_hiddens))\n",
    "Y = torch.ones((batch_size, num_kvpairs, num_hiddens))\n",
    "d2l.check_shape(attention(X, Y, Y, valid_lens),\n",
    "                (batch_size, num_queries, num_hiddens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a4e536",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "## 摘要\n",
    "\n",
    "多头注意力机制通过不同的查询、键和值的表示子空间来结合相同的注意力池化的知识。为了并行计算多头注意力的多个头，需要适当的张量操作。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 在此实验中可视化多个注意力头的权重。\n",
    "1. 假设我们有一个基于多头注意力训练好的模型，并且我们希望修剪不太重要的注意力头以提高预测速度。我们如何设计实验来衡量一个注意力头的重要性？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c502f7",
   "metadata": {
    "origin_pos": 26,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[讨论](https://discuss.d2l.ai/t/1635)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}