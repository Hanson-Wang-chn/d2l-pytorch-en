{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15193e65",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 自然语言处理：预训练\n",
    ":label:`chap_nlp_pretrain`\n",
    "\n",
    "\n",
    "人类需要交流。\n",
    "基于人类这一基本需求，每天都会产生大量的文本。\n",
    "鉴于社交媒体、聊天应用、电子邮件、产品评论、新闻文章、研究论文和书籍中的丰富文本，使计算机能够理解这些文本以便提供帮助或根据人类语言做出决策变得至关重要。\n",
    "\n",
    "*自然语言处理* 研究的是使用自然语言进行人机交互。\n",
    "在实践中，非常常见的是使用自然语言处理技术来处理和分析文本（人类自然语言）数据，例如 :numref:`sec_language-model` 中的语言模型和 :numref:`sec_machine_translation` 中的机器翻译模型。\n",
    "\n",
    "要理解文本，我们可以从学习其表示开始。\n",
    "利用大型语料库中现有的文本序列，\n",
    "*自监督学习* 已被广泛用于预训练文本表示，\n",
    "例如通过使用文本周围的部分内容来预测文本中隐藏的部分。\n",
    "这样，\n",
    "模型可以从 *大量* 文本数据中通过监督学习，\n",
    "而无需 *昂贵* 的标注工作！\n",
    "\n",
    "\n",
    "正如我们将在本章中看到的，\n",
    "当将每个词或子词视为单独的标记时，\n",
    "可以使用 word2vec、GloVe 或子词嵌入模型在大型语料库上预训练每个标记的表示。\n",
    "预训练后，每个标记的表示可以是一个向量，\n",
    "然而，无论上下文如何，它都保持不变。\n",
    "例如，“bank” 在\n",
    "“go to the bank to deposit some money”\n",
    "和\n",
    "“go to the bank to sit down”\n",
    "中的向量表示是相同的。\n",
    "因此，许多更近期的预训练模型会根据不同的上下文调整相同标记的表示。\n",
    "其中一个是 BERT，一个基于 Transformer 编码器的更深的自监督模型。\n",
    "在本章中，我们将重点介绍如何为文本预训练此类表示，\n",
    "如 :numref:`fig_nlp-map-pretrain` 所示。\n",
    "\n",
    "![预训练的文本表示可以输入各种深度学习架构以用于不同的下游自然语言处理应用。本章重点关注上游文本表示的预训练。](../img/nlp-map-pretrain.svg)\n",
    ":label:`fig_nlp-map-pretrain`\n",
    "\n",
    "\n",
    "为了纵观全局，\n",
    ":numref:`fig_nlp-map-pretrain` 显示了\n",
    "预训练的文本表示可以输入到各种深度学习架构中，以用于不同的下游自然语言处理应用。\n",
    "我们将在 :numref:`chap_nlp_app` 中介绍它们。\n",
    "\n",
    ":begin_tab:toc\n",
    " - [word2vec](word2vec.ipynb)\n",
    " - [approx-training](approx-training.ipynb)\n",
    " - [word-embedding-dataset](word-embedding-dataset.ipynb)\n",
    " - [word2vec-pretraining](word2vec-pretraining.ipynb)\n",
    " - [glove](glove.ipynb)\n",
    " - [subword-embedding](subword-embedding.ipynb)\n",
    " - [similarity-analogy](similarity-analogy.ipynb)\n",
    " - [bert](bert.ipynb)\n",
    " - [bert-dataset](bert-dataset.ipynb)\n",
    " - [bert-pretraining](bert-pretraining.ipynb)\n",
    ":end_tab:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}