{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2043d21",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Bidirectional Encoder Representations from Transformers (BERT)\n",
    ":label:`sec_bert`\n",
    "\n",
    "我们已经介绍了几种用于自然语言理解的词嵌入模型。预训练后，输出可以被视为一个矩阵，其中每一行都是表示预定义词汇表中一个词的向量。实际上，这些词嵌入模型都是*上下文无关*的。让我们从说明这一特性开始。\n",
    "\n",
    "## 从上下文无关到上下文相关\n",
    "\n",
    "回想在:numref:`sec_word2vec_pretraining`和:numref:`sec_synonyms`中的实验。例如，word2vec和GloVe无论单词的上下文（如果有）如何，都会为相同的单词分配相同的预训练向量。形式上，任何标记$x$的上下文无关表示是一个只以$x$作为输入的函数$f(x)$。鉴于自然语言中多义性和复杂语义的丰富性，上下文无关表示有明显的局限性。例如，“crane”在“a crane is flying”和“a crane driver came”这两个上下文中具有完全不同的含义；因此，同一个词可能会根据上下文被分配不同的表示。\n",
    "\n",
    "这促使了*上下文相关*词表示的发展，其中词的表示取决于它们的上下文。因此，标记$x$的上下文相关表示是依赖于$x$及其上下文$c(x)$的函数$f(x, c(x))$。流行的上下文相关表示包括TagLM（语言模型增强序列标注器）:cite:`Peters.Ammar.Bhagavatula.ea.2017`、CoVe（上下文向量）:cite:`McCann.Bradbury.Xiong.ea.2017`和ELMo（来自语言模型的嵌入）:cite:`Peters.Neumann.Iyyer.ea.2018`。\n",
    "\n",
    "例如，通过将整个序列作为输入，ELMo是一个函数，它为输入序列中的每个词分配一个表示。具体来说，ELMo结合了预训练双向LSTM的所有中间层表示作为输出表示。然后，ELMo表示将作为附加特征添加到下游任务现有的监督模型中，例如通过连接ELMo表示和现有模型中的原始表示（如GloVe）。一方面，在添加ELMo表示后，预训练双向LSTM模型中的所有权重都被冻结。另一方面，现有的监督模型是专门为给定任务定制的。利用当时针对不同任务的最佳模型，添加ELMo改进了六个自然语言处理任务：情感分析、自然语言推理、语义角色标注、共指消解、命名实体识别和问答。\n",
    "\n",
    "## 从任务特定到任务无关\n",
    "\n",
    "尽管ELMo显著改进了多种自然语言处理任务的解决方案，但每个解决方案仍然依赖于*任务特定*架构。然而，为每个自然语言处理任务设计一个特定架构在实际操作中并不简单。GPT（生成式预训练）模型代表了一种设计用于上下文相关表示的通用*任务无关*模型的努力:cite:`Radford.Narasimhan.Salimans.ea.2018`。基于Transformer解码器，GPT预训练了一个将用于表示文本序列的语言模型。当将GPT应用于下游任务时，语言模型的输出将被送入一个新增的线性输出层来预测任务的标签。与ELMo冻结预训练模型参数形成鲜明对比的是，GPT在下游任务的监督学习过程中对预训练Transformer解码器中的*所有*参数进行微调。GPT在十二个自然语言推理、问答、句子相似度和分类任务上进行了评估，并且在九个任务上以最小的模型架构变化改进了最先进的水平。\n",
    "\n",
    "然而，由于语言模型的自回归性质，GPT只能向前看（从左到右）。在“i went to the bank to deposit cash”和“i went to the bank to sit down”这两个上下文中，虽然“bank”对其左侧上下文敏感，GPT会为“bank”返回相同的表示，尽管它的含义不同。\n",
    "\n",
    "## BERT：结合两者之长\n",
    "\n",
    "正如我们所见，ELMo双向编码上下文但使用任务特定架构；而GPT是任务无关的，但仅从前向后编码上下文。结合两者的优点，BERT（来自Transformer的双向编码器表示）双向编码上下文，并且对于广泛范围内的自然语言处理任务只需要最小的架构更改:cite:`Devlin.Chang.Lee.ea.2018`。使用预训练的Transformer编码器，BERT能够基于其双向上下文表示任何标记。在下游任务的监督学习期间，BERT在两个方面类似于GPT。首先，BERT表示将被送入一个新增的输出层，根据任务的性质，模型架构的变化很小，例如预测每个标记与预测整个序列。其次，预训练Transformer编码器的所有参数都将被微调，而额外的输出层将从头开始训练。:numref:`fig_elmo-gpt-bert`描绘了ELMo、GPT和BERT之间的差异。\n",
    "\n",
    "![A comparison of ELMo, GPT, and BERT.](../img/elmo-gpt-bert.svg)\n",
    ":label:`fig_elmo-gpt-bert`\n",
    "\n",
    "BERT进一步改进了十一项自然语言处理任务的最先进水平，涵盖了广泛的类别：(i) 单文本分类（例如，情感分析），(ii) 文本对分类（例如，自然语言推理），(iii) 问答，(iv) 文本标注（例如，命名实体识别）。从2018年提出的上下文相关的ELMo到任务无关的GPT和BERT，概念简单但经验强大的深度表示预训练彻底改变了各种自然语言处理任务的解决方案。\n",
    "\n",
    "在本章的其余部分，我们将深入探讨BERT的预训练。当在:numref:`chap_nlp_app`解释自然语言处理应用时，我们将说明BERT在下游应用中的微调。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffa5c8df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:51.331685Z",
     "iopub.status.busy": "2023-08-18T19:31:51.331049Z",
     "iopub.status.idle": "2023-08-18T19:31:54.812815Z",
     "shell.execute_reply": "2023-08-18T19:31:54.811897Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86eb35f",
   "metadata": {
    "origin_pos": 3
   },
   "source": [
    "## [**输入表示**]\n",
    ":label:`subsec_bert_input_rep`\n",
    "\n",
    "在自然语言处理中，\n",
    "一些任务（例如，情感分析）以单个文本作为输入，\n",
    "而在其他一些任务（例如，自然语言推理）中，\n",
    "输入是一对文本序列。\n",
    "BERT 输入序列明确地表示了单个文本和文本对。\n",
    "在前者中，\n",
    "BERT 输入序列是特殊分类标记“&lt;cls&gt;”、\n",
    "一个文本序列的标记以及特殊分隔标记“&lt;sep&gt;”的连接。\n",
    "在后者中，\n",
    "BERT 输入序列是“&lt;cls&gt;”、第一个文本序列的标记、\n",
    "“&lt;sep&gt;”、第二个文本序列的标记以及“&lt;sep&gt;”的连接。\n",
    "我们将始终区分术语“BERT 输入序列”\n",
    "与其他类型的“序列”。\n",
    "例如，一个 *BERT 输入序列* 可能包括一个 *文本序列* 或两个 *文本序列*。\n",
    "\n",
    "为了区分文本对，\n",
    "学习到的段嵌入 $\\mathbf{e}_A$ 和 $\\mathbf{e}_B$\n",
    "分别添加到第一个序列和第二个序列的标记嵌入中。\n",
    "对于单个文本输入，只使用 $\\mathbf{e}_A$。\n",
    "\n",
    "以下 `get_tokens_and_segments` 函数接受一个或两个句子\n",
    "作为输入，然后返回 BERT 输入序列的标记\n",
    "及其对应的段 ID。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c018a43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:54.816440Z",
     "iopub.status.busy": "2023-08-18T19:31:54.816038Z",
     "iopub.status.idle": "2023-08-18T19:31:54.823105Z",
     "shell.execute_reply": "2023-08-18T19:31:54.822123Z"
    },
    "origin_pos": 4,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def get_tokens_and_segments(tokens_a, tokens_b=None):\n",
    "    \"\"\"Get tokens of the BERT input sequence and their segment IDs.\"\"\"\n",
    "    tokens = ['<cls>'] + tokens_a + ['<sep>']\n",
    "    # 0 and 1 are marking segment A and B, respectively\n",
    "    segments = [0] * (len(tokens_a) + 2)\n",
    "    if tokens_b is not None:\n",
    "        tokens += tokens_b + ['<sep>']\n",
    "        segments += [1] * (len(tokens_b) + 1)\n",
    "    return tokens, segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948cea11",
   "metadata": {
    "origin_pos": 5
   },
   "source": [
    "BERT 选择了 Transformer 编码器作为其双向架构。\n",
    "在 Transformer 编码器中，\n",
    "位置嵌入被添加到 BERT 输入序列的每个位置。\n",
    "然而，与原始的 Transformer 编码器不同，\n",
    "BERT 使用*可学习的*位置嵌入。\n",
    "总而言之，:numref:`fig_bert-input` 显示\n",
    "BERT 输入序列的嵌入是\n",
    "词嵌入、段嵌入和位置嵌入的总和。\n",
    "\n",
    "![The embeddings of the BERT input sequence are the sum\n",
    "of the token embeddings, segment embeddings, and positional embeddings.](../img/bert-input.svg)\n",
    ":label:`fig_bert-input`\n",
    "\n",
    "以下 [**`BERTEncoder` 类**] 与 :numref:`sec_transformer` 中实现的 `TransformerEncoder` 类相似。\n",
    "与 `TransformerEncoder` 不同，`BERTEncoder` 使用\n",
    "段嵌入和可学习的位置嵌入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a90baa53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:54.828159Z",
     "iopub.status.busy": "2023-08-18T19:31:54.827512Z",
     "iopub.status.idle": "2023-08-18T19:31:54.835645Z",
     "shell.execute_reply": "2023-08-18T19:31:54.834698Z"
    },
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "class BERTEncoder(nn.Module):\n",
    "    \"\"\"BERT encoder.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\n",
    "                 num_blks, dropout, max_len=1000, **kwargs):\n",
    "        super(BERTEncoder, self).__init__(**kwargs)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(f\"{i}\", d2l.TransformerEncoderBlock(\n",
    "                num_hiddens, ffn_num_hiddens, num_heads, dropout, True))\n",
    "        # In BERT, positional embeddings are learnable, thus we create a\n",
    "        # parameter of positional embeddings that are long enough\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len,\n",
    "                                                      num_hiddens))\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens):\n",
    "        # Shape of `X` remains unchanged in the following code snippet:\n",
    "        # (batch size, max sequence length, `num_hiddens`)\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "        X = X + self.pos_embedding[:, :X.shape[1], :]\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, valid_lens)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab99ce42",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "假设词汇量为10000。\n",
    "为了演示[**`BERTEncoder`的前向推理**]，\n",
    "让我们创建它的实例并初始化其参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eb45fe4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:54.840002Z",
     "iopub.status.busy": "2023-08-18T19:31:54.839108Z",
     "iopub.status.idle": "2023-08-18T19:31:54.985512Z",
     "shell.execute_reply": "2023-08-18T19:31:54.984301Z"
    },
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4\n",
    "ffn_num_input, num_blks, dropout = 768, 2, 0.2\n",
    "encoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\n",
    "                      num_blks, dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c52eb2f",
   "metadata": {
    "origin_pos": 11
   },
   "source": [
    "我们将`tokens`定义为长度为8的2个BERT输入序列，其中每个token是词汇表中的一个索引。`BERTEncoder`对输入`tokens`进行前向推理，返回编码结果，其中每个token由一个向量表示，该向量的长度由超参数`num_hiddens`预定义。这个超参数通常被称为Transformer编码器的*隐藏大小*（隐藏单元的数量）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56903d71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:54.990720Z",
     "iopub.status.busy": "2023-08-18T19:31:54.989911Z",
     "iopub.status.idle": "2023-08-18T19:31:55.148261Z",
     "shell.execute_reply": "2023-08-18T19:31:55.147176Z"
    },
    "origin_pos": 13,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = torch.randint(0, vocab_size, (2, 8))\n",
    "segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])\n",
    "encoded_X = encoder(tokens, segments, None)\n",
    "encoded_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09881b5d",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "## 预训练任务\n",
    ":label:`subsec_bert_pretraining_tasks`\n",
    "\n",
    "`BERTEncoder` 的前向推理给出了输入文本中每个标记以及插入的特殊标记“&lt;cls&gt;”和“&lt;seq&gt;”的BERT表示。接下来，我们将使用这些表示来计算预训练BERT的损失函数。预训练由以下两个任务组成：掩码语言模型和下一句预测。\n",
    "\n",
    "### [**掩码语言模型**]\n",
    ":label:`subsec_mlm`\n",
    "\n",
    "如 :numref:`sec_language-model` 所示，语言模型使用其左侧的上下文来预测一个标记。为了在表示每个标记时编码双向上下文，BERT随机遮蔽标记，并以自监督的方式使用双向上下文中的标记来预测被遮蔽的标记。这个任务被称为*掩码语言模型*。\n",
    "\n",
    "在这个预训练任务中，15%的标记将被随机选为被遮蔽的标记进行预测。为了在不利用标签的情况下预测一个被遮蔽的标记，一种直接的方法是在BERT输入序列中始终将其替换为特殊的“&lt;mask&gt;”标记。然而，这种人为的特殊标记“&lt;mask&gt;”在微调时永远不会出现。为了避免预训练和微调之间的这种不匹配，如果一个标记被遮蔽用于预测（例如，“great”在“this movie is great”中被选为遮蔽并预测），在输入中它将被替换为：\n",
    "\n",
    "* 80%的时间用特殊的“&lt;mask&gt;”标记（例如，“this movie is great”变成“this movie is &lt;mask&gt;”）；\n",
    "* 10%的时间用一个随机标记（例如，“this movie is great”变成“this movie is drink”）；\n",
    "* 10%的时间保持不变的标签标记（例如，“this movie is great”变成“this movie is great”）。\n",
    "\n",
    "注意，15%中有10%的时间会插入一个随机标记。这种偶尔的噪声鼓励BERT在其双向上下文编码中减少对被遮蔽标记（尤其是当标签标记保持不变时）的偏见。\n",
    "\n",
    "我们实现以下 `MaskLM` 类来预测BERT预训练中的掩码语言模型任务中的被遮蔽标记。预测使用了一个单隐藏层的MLP (`self.mlp`)。在前向推理中，它接受两个输入：`BERTEncoder` 的编码结果和用于预测的标记位置。输出是这些位置上的预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8614e46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:55.154295Z",
     "iopub.status.busy": "2023-08-18T19:31:55.153400Z",
     "iopub.status.idle": "2023-08-18T19:31:55.162155Z",
     "shell.execute_reply": "2023-08-18T19:31:55.161271Z"
    },
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "class MaskLM(nn.Module):\n",
    "    \"\"\"The masked language model task of BERT.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, **kwargs):\n",
    "        super(MaskLM, self).__init__(**kwargs)\n",
    "        self.mlp = nn.Sequential(nn.LazyLinear(num_hiddens),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.LayerNorm(num_hiddens),\n",
    "                                 nn.LazyLinear(vocab_size))\n",
    "\n",
    "    def forward(self, X, pred_positions):\n",
    "        num_pred_positions = pred_positions.shape[1]\n",
    "        pred_positions = pred_positions.reshape(-1)\n",
    "        batch_size = X.shape[0]\n",
    "        batch_idx = torch.arange(0, batch_size)\n",
    "        # Suppose that `batch_size` = 2, `num_pred_positions` = 3, then\n",
    "        # `batch_idx` is `torch.tensor([0, 0, 0, 1, 1, 1])`\n",
    "        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)\n",
    "        masked_X = X[batch_idx, pred_positions]\n",
    "        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n",
    "        mlm_Y_hat = self.mlp(masked_X)\n",
    "        return mlm_Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce1f5c9",
   "metadata": {
    "origin_pos": 17
   },
   "source": [
    "为了展示[**`MaskLM`的前向推理**]，我们创建其实例`mlm`并进行初始化。回想一下，来自`BERTEncoder`前向推理的`encoded_X`代表2个BERT输入序列。我们将`mlm_positions`定义为在`encoded_X`任一BERT输入序列中要预测的3个索引。`mlm`的前向推理返回`encoded_X`所有被遮蔽位置`mlm_positions`的预测结果`mlm_Y_hat`。对于每个预测，结果的大小等于词汇表的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b3fc7d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:55.166188Z",
     "iopub.status.busy": "2023-08-18T19:31:55.165836Z",
     "iopub.status.idle": "2023-08-18T19:31:55.273706Z",
     "shell.execute_reply": "2023-08-18T19:31:55.272680Z"
    },
    "origin_pos": 19,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm = MaskLM(vocab_size, num_hiddens)\n",
    "mlm_positions = torch.tensor([[1, 5, 2], [6, 1, 5]])\n",
    "mlm_Y_hat = mlm(encoded_X, mlm_positions)\n",
    "mlm_Y_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd88359",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "利用被遮罩的预测词元`mlm_Y_hat`的真实标签`mlm_Y`，我们可以计算BERT预训练中掩码语言模型任务的交叉熵损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d85768b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:55.278830Z",
     "iopub.status.busy": "2023-08-18T19:31:55.278109Z",
     "iopub.status.idle": "2023-08-18T19:31:55.288546Z",
     "shell.execute_reply": "2023-08-18T19:31:55.287485Z"
    },
    "origin_pos": 22,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_Y = torch.tensor([[7, 8, 9], [10, 20, 30]])\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))\n",
    "mlm_l.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78b8c02",
   "metadata": {
    "origin_pos": 23
   },
   "source": [
    "### [**下一句预测**]\n",
    ":label:`subsec_nsp`\n",
    "\n",
    "尽管掩码语言模型能够编码双向上下文来表示词语，但它并没有显式地建模文本对之间的逻辑关系。为了帮助理解两个文本序列之间的关系，BERT在其预训练中考虑了一个二分类任务，即*下一句预测*。在生成用于预训练的句子对时，有一半的时间它们确实是连续的句子，并标记为“True”；而在另一半时间里，第二个句子是从语料库中随机抽取的，并标记为“False”。\n",
    "\n",
    "下面的`NextSentencePred`类使用一个单隐藏层的MLP来预测在BERT输入序列中第二个句子是否是第一个句子的下一句。由于Transformer编码器中的自注意力机制，特殊标记“&lt;cls&gt;”的BERT表示编码了输入中的两个句子。因此，MLP分类器的输出层（`self.output`）将`X`作为输入，其中`X`是MLP隐藏层的输出，该隐藏层的输入是编码后的“&lt;cls&gt;”标记。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1951876",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:55.292806Z",
     "iopub.status.busy": "2023-08-18T19:31:55.291904Z",
     "iopub.status.idle": "2023-08-18T19:31:55.298328Z",
     "shell.execute_reply": "2023-08-18T19:31:55.297464Z"
    },
    "origin_pos": 25,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "class NextSentencePred(nn.Module):\n",
    "    \"\"\"The next sentence prediction task of BERT.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(NextSentencePred, self).__init__(**kwargs)\n",
    "        self.output = nn.LazyLinear(2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # `X` shape: (batch size, `num_hiddens`)\n",
    "        return self.output(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f9f8eb",
   "metadata": {
    "origin_pos": 26
   },
   "source": [
    "我们可以看到，[**`NextSentencePred`的前向推理**]实例对每个BERT输入序列返回二元预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aba0cce5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:55.302539Z",
     "iopub.status.busy": "2023-08-18T19:31:55.301869Z",
     "iopub.status.idle": "2023-08-18T19:31:55.310590Z",
     "shell.execute_reply": "2023-08-18T19:31:55.309427Z"
    },
    "origin_pos": 28,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch by default will not flatten the tensor as seen in mxnet where, if\n",
    "# flatten=True, all but the first axis of input data are collapsed together\n",
    "encoded_X = torch.flatten(encoded_X, start_dim=1)\n",
    "# input_shape for NSP: (batch size, `num_hiddens`)\n",
    "nsp = NextSentencePred()\n",
    "nsp_Y_hat = nsp(encoded_X)\n",
    "nsp_Y_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aacce83",
   "metadata": {
    "origin_pos": 29
   },
   "source": [
    "这两个二分类的交叉熵损失也可以计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba504299",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:55.314489Z",
     "iopub.status.busy": "2023-08-18T19:31:55.313852Z",
     "iopub.status.idle": "2023-08-18T19:31:55.321256Z",
     "shell.execute_reply": "2023-08-18T19:31:55.320193Z"
    },
    "origin_pos": 31,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsp_y = torch.tensor([0, 1])\n",
    "nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "nsp_l.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ac0df2",
   "metadata": {
    "origin_pos": 32
   },
   "source": [
    "值得注意的是，上述所有预训练任务中的标签都可以从预训练语料库中轻易获得，无需人工标注。原始的BERT是在BookCorpus :cite:`Zhu.Kiros.Zemel.ea.2015`和英文维基百科的合并文本上进行预训练的。这两个文本语料库非常庞大：它们分别包含8亿词和25亿词。\n",
    "\n",
    "## [**综合起来**]\n",
    "\n",
    "在预训练BERT时，最终的损失函数是掩码语言模型和下一句预测两个损失函数的线性组合。现在我们可以通过实例化`BERTEncoder`、`MaskLM`和`NextSentencePred`这三个类来定义`BERTModel`类。前向推理返回编码后的BERT表示`encoded_X`、掩码语言模型的预测`mlm_Y_hat`以及下一句预测`nsp_Y_hat`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73c331cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:31:55.325106Z",
     "iopub.status.busy": "2023-08-18T19:31:55.324530Z",
     "iopub.status.idle": "2023-08-18T19:31:55.333301Z",
     "shell.execute_reply": "2023-08-18T19:31:55.332018Z"
    },
    "origin_pos": 34,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "class BERTModel(nn.Module):\n",
    "    \"\"\"The BERT model.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens,\n",
    "                 num_heads, num_blks, dropout, max_len=1000):\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.encoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens,\n",
    "                                   num_heads, num_blks, dropout,\n",
    "                                   max_len=max_len)\n",
    "        self.hidden = nn.Sequential(nn.LazyLinear(num_hiddens),\n",
    "                                    nn.Tanh())\n",
    "        self.mlm = MaskLM(vocab_size, num_hiddens)\n",
    "        self.nsp = NextSentencePred()\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens=None, pred_positions=None):\n",
    "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
    "        if pred_positions is not None:\n",
    "            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "        # The hidden layer of the MLP classifier for next sentence prediction.\n",
    "        # 0 is the index of the '<cls>' token\n",
    "        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))\n",
    "        return encoded_X, mlm_Y_hat, nsp_Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be55112a",
   "metadata": {
    "origin_pos": 35
   },
   "source": [
    "## 摘要\n",
    "\n",
    "* 诸如word2vec和GloVe之类的词嵌入模型是上下文无关的。无论单词的上下文如何（如果有），它们都会为相同的单词分配相同的预训练向量。对于处理自然语言中的多义性或复杂语义来说，这很困难。\n",
    "* 对于像ELMo和GPT这样的上下文敏感的词表示方法，词语的表示取决于其上下文。\n",
    "* ELMo双向编码上下文，但使用特定任务架构（然而，为每个自然语言处理任务设计一个特定架构实际上并不容易）；而GPT与任务无关，但仅从左到右编码上下文。\n",
    "* BERT结合了两者的优点：它双向编码上下文，并且在广泛的自然语言处理任务中只需要最小的架构更改。\n",
    "* BERT输入序列的嵌入是词嵌入、段嵌入和位置嵌入的总和。\n",
    "* BERT的预训练由两个任务组成：掩码语言建模和下一句预测。前者能够为表示词语编码双向上下文，而后者则明确地对文本对之间的逻辑关系进行建模。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 在其他条件相同的情况下，掩码语言模型相比于从左到右的语言模型，在收敛时需要更多的还是更少的预训练步骤？为什么？\n",
    "1. 在BERT的原始实现中，`BERTEncoder`（通过`d2l.TransformerEncoderBlock`）中的逐位置前馈网络和`MaskLM`中的全连接层都使用高斯误差线性单元（GELU）:cite:`Hendrycks.Gimpel.2016`作为激活函数。研究GELU与ReLU之间的差异。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9727cff7",
   "metadata": {
    "origin_pos": 37,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[讨论](https://discuss.d2l.ai/t/1490)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}