{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d02c5b",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 转置卷积\n",
    ":label:`sec_transposed_conv`\n",
    "\n",
    "到目前为止我们所见过的CNN层，例如卷积层（:numref:`sec_conv_layer`）和池化层（:numref:`sec_pooling`），通常会减少（下采样）输入的空间维度（高度和宽度），或者保持它们不变。在像素级分类的语义分割中，如果输入和输出的空间维度相同将非常方便。例如，一个输出像素的通道维度可以保存同一空间位置输入像素的分类结果。\n",
    "\n",
    "为了实现这一点，尤其是在CNN层减少了空间维度之后，我们可以使用另一种类型的CNN层来增加（上采样）中间特征图的空间维度。在本节中，我们将介绍*转置卷积*，也称为*分数步长卷积* :cite:`Dumoulin.Visin.2016`，用于通过卷积操作逆转下采样操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64ac86cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:40:30.453582Z",
     "iopub.status.busy": "2023-08-18T19:40:30.453269Z",
     "iopub.status.idle": "2023-08-18T19:40:33.572095Z",
     "shell.execute_reply": "2023-08-18T19:40:33.570416Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e53c32",
   "metadata": {
    "origin_pos": 3
   },
   "source": [
    "## 基本操作\n",
    "\n",
    "暂时忽略通道，\n",
    "让我们从\n",
    "步幅为1且无填充的基本转置卷积操作开始。\n",
    "假设我们有一个\n",
    "$n_h \\times n_w$ 的输入张量\n",
    "和一个 $k_h \\times k_w$ 的内核。\n",
    "以步幅为1在每行滑动内核窗口\n",
    "$n_w$ 次\n",
    "并在每列滑动\n",
    "$n_h$ 次\n",
    "产生\n",
    "总共 $n_h n_w$ 个中间结果。\n",
    "每个中间结果是\n",
    "一个 $(n_h + k_h - 1) \\times (n_w + k_w - 1)$\n",
    "初始化为零的张量。\n",
    "为了计算每个中间张量，\n",
    "输入张量中的每个元素\n",
    "与内核相乘\n",
    "使得生成的 $k_h \\times k_w$ 张量\n",
    "替换每个中间张量的一部分。\n",
    "请注意\n",
    "每个中间张量中被替换部分的位置\n",
    "对应于用于计算的输入张量中的元素位置。\n",
    "最后，所有中间结果\n",
    "相加得到输出。\n",
    "\n",
    "例如，\n",
    ":numref:`fig_trans_conv` 说明了\n",
    "如何使用 $2\\times 2$ 内核对 $2\\times 2$ 输入张量进行转置卷积计算。\n",
    "\n",
    "\n",
    "![Transposed convolution with a $2\\times 2$ kernel. The shaded portions are a portion of an intermediate tensor as well as the input and kernel tensor elements used for the  computation.](../img/trans_conv.svg)\n",
    ":label:`fig_trans_conv`\n",
    "\n",
    "\n",
    "我们可以（**实现这个基本的转置卷积操作**）`trans_conv` 对输入矩阵 `X` 和内核矩阵 `K` 进行操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d01fcf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:40:33.580872Z",
     "iopub.status.busy": "2023-08-18T19:40:33.579987Z",
     "iopub.status.idle": "2023-08-18T19:40:33.594490Z",
     "shell.execute_reply": "2023-08-18T19:40:33.593361Z"
    },
    "origin_pos": 4,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def trans_conv(X, K):\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] + h - 1, X.shape[1] + w - 1))\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            Y[i: i + h, j: j + w] += X[i, j] * K\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33e6143",
   "metadata": {
    "origin_pos": 5
   },
   "source": [
    "与常规卷积（在 :numref:`sec_conv_layer` 中）通过核*减少*输入元素不同，转置卷积通过核*广播*输入元素，从而产生比输入更大的输出。我们可以从 :numref:`fig_trans_conv` 构建输入张量 `X` 和核张量 `K` 来[**验证上述基本二维转置卷积操作的实现输出**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75ed0b6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:40:33.598934Z",
     "iopub.status.busy": "2023-08-18T19:40:33.598651Z",
     "iopub.status.idle": "2023-08-18T19:40:33.626958Z",
     "shell.execute_reply": "2023-08-18T19:40:33.625781Z"
    },
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  1.],\n",
       "        [ 0.,  4.,  6.],\n",
       "        [ 4., 12.,  9.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "trans_conv(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060319a4",
   "metadata": {
    "origin_pos": 7
   },
   "source": [
    "或者，当输入 `X` 和核 `K` 都是四维张量时，我们可以[**使用高级API来获得相同的结果**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1ad37e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:40:33.632024Z",
     "iopub.status.busy": "2023-08-18T19:40:33.631545Z",
     "iopub.status.idle": "2023-08-18T19:40:33.647991Z",
     "shell.execute_reply": "2023-08-18T19:40:33.646736Z"
    },
    "origin_pos": 9,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  0.,  1.],\n",
       "          [ 0.,  4.,  6.],\n",
       "          [ 4., 12.,  9.]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, K = X.reshape(1, 1, 2, 2), K.reshape(1, 1, 2, 2)\n",
    "tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, bias=False)\n",
    "tconv.weight.data = K\n",
    "tconv(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53afbb91",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "## [**填充、步幅和多通道**]\n",
    "\n",
    "不同于常规卷积中\n",
    "对输入应用填充，\n",
    "在转置卷积中\n",
    "是对输出应用填充。\n",
    "例如，\n",
    "当指定高度和宽度两侧的\n",
    "填充数为1时，\n",
    "转置卷积输出的第一行和最后一行、\n",
    "第一列和最后一列将被移除。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1048beb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:40:33.653300Z",
     "iopub.status.busy": "2023-08-18T19:40:33.652500Z",
     "iopub.status.idle": "2023-08-18T19:40:33.662731Z",
     "shell.execute_reply": "2023-08-18T19:40:33.661823Z"
    },
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[4.]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, padding=1, bias=False)\n",
    "tconv.weight.data = K\n",
    "tconv(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549fc8f2",
   "metadata": {
    "origin_pos": 13
   },
   "source": [
    "在转置卷积中，步幅是为中间结果（因此也是输出）指定的，而不是为输入指定的。使用:numref:`fig_trans_conv`中的相同输入和核张量，将步幅从1改为2会增加中间张量的高度和宽度，因此输出张量如:numref:`fig_trans_conv_stride2`所示。\n",
    "\n",
    "![带有$2\\times 2$核的转置卷积，步幅为2。阴影部分是中间张量的一部分以及用于计算的输入和核张量元素。](../img/trans_conv_stride2.svg)\n",
    ":label:`fig_trans_conv_stride2`\n",
    "\n",
    "\n",
    "\n",
    "以下代码片段可以验证:numref:`fig_trans_conv_stride2`中步幅为2的转置卷积输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72cccf5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:40:33.667420Z",
     "iopub.status.busy": "2023-08-18T19:40:33.666693Z",
     "iopub.status.idle": "2023-08-18T19:40:33.676004Z",
     "shell.execute_reply": "2023-08-18T19:40:33.675089Z"
    },
    "origin_pos": 15,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0., 1.],\n",
       "          [0., 0., 2., 3.],\n",
       "          [0., 2., 0., 3.],\n",
       "          [4., 6., 6., 9.]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2, bias=False)\n",
    "tconv.weight.data = K\n",
    "tconv(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecbf916",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "对于多个输入和输出通道，转置卷积的工作方式与常规卷积相同。假设输入有 $c_i$ 个通道，并且转置卷积为每个输入通道分配一个 $k_h\\times k_w$ 的核张量。当指定多个输出通道时，每个输出通道将有一个 $c_i\\times k_h\\times k_w$ 的核。\n",
    "\n",
    "总之，如果我们把 $\\mathsf{X}$ 输入到卷积层 $f$ 中以输出 $\\mathsf{Y}=f(\\mathsf{X})$，并创建一个除了输出通道数等于 $\\mathsf{X}$ 的通道数之外其他超参数都与 $f$ 相同的转置卷积层 $g$，那么 $g(Y)$ 将具有与 $\\mathsf{X}$ 相同的形状。这可以通过以下示例来说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f8aac99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:40:33.679264Z",
     "iopub.status.busy": "2023-08-18T19:40:33.678924Z",
     "iopub.status.idle": "2023-08-18T19:40:33.724131Z",
     "shell.execute_reply": "2023-08-18T19:40:33.723022Z"
    },
    "origin_pos": 18,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 10, 16, 16))\n",
    "conv = nn.Conv2d(10, 20, kernel_size=5, padding=2, stride=3)\n",
    "tconv = nn.ConvTranspose2d(20, 10, kernel_size=5, padding=2, stride=3)\n",
    "tconv(conv(X)).shape == X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65785832",
   "metadata": {
    "origin_pos": 19
   },
   "source": [
    "## [**与矩阵转置的联系**]\n",
    ":label:`subsec-connection-to-mat-transposition`\n",
    "\n",
    "转置卷积之所以得名，是因为它与矩阵转置有关。为了说明这一点，我们首先来看看如何使用矩阵乘法来实现卷积。在下面的例子中，我们定义了一个 $3\\times 3$ 的输入 `X` 和一个 $2\\times 2$ 的卷积核 `K`，然后使用 `corr2d` 函数来计算卷积输出 `Y`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54c1abd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:40:33.727935Z",
     "iopub.status.busy": "2023-08-18T19:40:33.727252Z",
     "iopub.status.idle": "2023-08-18T19:40:33.735227Z",
     "shell.execute_reply": "2023-08-18T19:40:33.734426Z"
    },
    "origin_pos": 20,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[27., 37.],\n",
       "        [57., 67.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(9.0).reshape(3, 3)\n",
    "K = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "Y = d2l.corr2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383ed5e7",
   "metadata": {
    "origin_pos": 21
   },
   "source": [
    "接下来，我们将卷积核 `K` 重写为一个包含许多零的稀疏权重矩阵 `W`。权重矩阵的形状是（$4$，$9$），其中非零元素来自卷积核 `K`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c83e2aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:40:33.738704Z",
     "iopub.status.busy": "2023-08-18T19:40:33.738150Z",
     "iopub.status.idle": "2023-08-18T19:40:33.746690Z",
     "shell.execute_reply": "2023-08-18T19:40:33.745684Z"
    },
    "origin_pos": 22,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 0., 3., 4., 0., 0., 0., 0.],\n",
       "        [0., 1., 2., 0., 3., 4., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 2., 0., 3., 4., 0.],\n",
       "        [0., 0., 0., 0., 1., 2., 0., 3., 4.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def kernel2matrix(K):\n",
    "    k, W = torch.zeros(5), torch.zeros((4, 9))\n",
    "    k[:2], k[3:5] = K[0, :], K[1, :]\n",
    "    W[0, :5], W[1, 1:6], W[2, 3:8], W[3, 4:] = k, k, k, k\n",
    "    return W\n",
    "\n",
    "W = kernel2matrix(K)\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125346be",
   "metadata": {
    "origin_pos": 23
   },
   "source": [
    "将输入 `X` 按行连接成一个长度为9的向量。然后 `W` 和向量化后的 `X` 的矩阵乘法得到一个长度为4的向量。\n",
    "经过重塑后，我们可以从上面的原始卷积操作中获得相同的结果 `Y`：\n",
    "我们刚刚使用矩阵乘法实现了卷积。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "444dbc7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:40:33.750344Z",
     "iopub.status.busy": "2023-08-18T19:40:33.749752Z",
     "iopub.status.idle": "2023-08-18T19:40:33.757265Z",
     "shell.execute_reply": "2023-08-18T19:40:33.756389Z"
    },
    "origin_pos": 24,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True],\n",
       "        [True, True]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y == torch.matmul(W, X.reshape(-1)).reshape(2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f470126e",
   "metadata": {
    "origin_pos": 25
   },
   "source": [
    "同样，我们可以使用矩阵乘法来实现转置卷积。在下面的例子中，我们将上述常规卷积的 $2 \\times 2$ 输出 `Y` 作为转置卷积的输入。为了通过矩阵乘法实现这一操作，我们只需要将权重矩阵 `W` 转置为新的形状 $(9, 4)$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1834374",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:40:33.761069Z",
     "iopub.status.busy": "2023-08-18T19:40:33.760455Z",
     "iopub.status.idle": "2023-08-18T19:40:33.767618Z",
     "shell.execute_reply": "2023-08-18T19:40:33.766760Z"
    },
    "origin_pos": 26,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = trans_conv(Y, K)\n",
    "Z == torch.matmul(W.T, Y.reshape(-1)).reshape(3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08875952",
   "metadata": {
    "origin_pos": 27
   },
   "source": [
    "考虑通过矩阵乘法来实现卷积。\n",
    "给定一个输入向量 $\\mathbf{x}$\n",
    "和一个权重矩阵 $\\mathbf{W}$，\n",
    "卷积的前向传播函数\n",
    "可以通过将其输入与权重矩阵相乘\n",
    "并输出一个向量\n",
    "$\\mathbf{y}=\\mathbf{W}\\mathbf{x}$ 来实现。\n",
    "由于反向传播\n",
    "遵循链式法则\n",
    "且 $\\nabla_{\\mathbf{x}}\\mathbf{y}=\\mathbf{W}^\\top$，\n",
    "卷积的反向传播函数\n",
    "可以通过将其输入与\n",
    "转置后的权重矩阵 $\\mathbf{W}^\\top$ 相乘来实现。\n",
    "因此，\n",
    "转置卷积层\n",
    "可以简单地交换卷积层的前向传播函数\n",
    "和反向传播函数：\n",
    "其前向传播\n",
    "和反向传播函数\n",
    "分别将它们的输入向量与\n",
    "$\\mathbf{W}^\\top$ 和 $\\mathbf{W}$ 相乘。\n",
    "\n",
    "## 摘要\n",
    "\n",
    "* 与通过内核减少输入元素的常规卷积不同，转置卷积通过内核广播输入元素，从而产生比输入更大的输出。\n",
    "* 如果我们将 $\\mathsf{X}$ 输入到一个卷积层 $f$ 中以输出 $\\mathsf{Y}=f(\\mathsf{X})$ 并创建一个除了输出通道数为 $\\mathsf{X}$ 的通道数之外具有相同超参数的转置卷积层 $g$，那么 $g(Y)$ 将具有与 $\\mathsf{X}$ 相同的形状。\n",
    "* 我们可以使用矩阵乘法来实现卷积。转置卷积层可以简单地交换卷积层的前向传播函数和反向传播函数。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 在 :numref:`subsec-connection-to-mat-transposition` 中，卷积输入 `X` 和转置卷积输出 `Z` 具有相同的形状。它们的值也相同吗？为什么？\n",
    "1. 使用矩阵乘法来实现卷积是否高效？为什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bcc93d",
   "metadata": {
    "origin_pos": 29,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[讨论](https://discuss.d2l.ai/t/1450)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}