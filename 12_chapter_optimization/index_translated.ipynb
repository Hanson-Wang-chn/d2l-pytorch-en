{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b9b78fd",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 优化算法\n",
    ":label:`chap_optimization`\n",
    "\n",
    "如果你按顺序读这本书到目前为止，你已经使用了多种优化算法来训练深度学习模型。它们是让你能够不断更新模型参数并最小化训练集上损失函数值的工具。实际上，对于那些满足于将优化视为在简单环境中最小化目标函数的黑盒设备的人来说，知道存在一系列这样的程序（如“SGD”和“Adam”）可能就足够了。\n",
    "\n",
    "然而，要做好这一点，需要更深入的知识。优化算法对深度学习至关重要。一方面，训练一个复杂的深度学习模型可能需要数小时、数天甚至数周的时间。优化算法的性能直接影响模型的训练效率。另一方面，理解不同优化算法的原理及其超参数的作用，可以让我们有针对性地调整超参数，以提高深度学习模型的性能。\n",
    "\n",
    "在本章中，我们将深入探讨常见的深度学习优化算法。几乎所有出现在深度学习中的优化问题都是*非凸*的。尽管如此，在*凸*问题背景下设计和分析算法已经被证明是非常有启发性的。因此，本章包括关于凸优化的入门知识以及在凸目标函数上非常简单的随机梯度下降算法的证明。\n",
    "\n",
    ":begin_tab:toc\n",
    " - [optimization-intro](optimization-intro.ipynb)\n",
    " - [convexity](convexity.ipynb)\n",
    " - [gd](gd.ipynb)\n",
    " - [sgd](sgd.ipynb)\n",
    " - [minibatch-sgd](minibatch-sgd.ipynb)\n",
    " - [momentum](momentum.ipynb)\n",
    " - [adagrad](adagrad.ipynb)\n",
    " - [rmsprop](rmsprop.ipynb)\n",
    " - [adadelta](adadelta.ipynb)\n",
    " - [adam](adam.ipynb)\n",
    " - [lr-scheduler](lr-scheduler.ipynb)\n",
    ":end_tab:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}