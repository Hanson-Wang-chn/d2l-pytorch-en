{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68096c17",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 自然语言推理和数据集\n",
    ":label:`sec_natural-language-inference-and-dataset`\n",
    "\n",
    "在 :numref:`sec_sentiment` 中，我们讨论了情感分析的问题。\n",
    "该任务旨在将单个文本序列分类到预定义的类别中，\n",
    "例如一组情感极性。\n",
    "然而，当需要决定一个句子是否可以从另一个句子推导出来，\n",
    "或者通过识别语义等价的句子来消除冗余时，\n",
    "只知道如何分类一个文本序列是不够的。\n",
    "相反，我们需要能够对成对的文本序列进行推理。\n",
    "\n",
    "## 自然语言推理\n",
    "\n",
    "*自然语言推理* 研究一个 *假设* 是否可以从前提推导出来，其中两者都是文本序列。\n",
    "换句话说，自然语言推理确定一对文本序列之间的逻辑关系。\n",
    "这种关系通常分为三种类型：\n",
    "\n",
    "* *蕴含*：可以从前提中推导出假设。\n",
    "* *矛盾*：可以从前提中推导出假设的否定。\n",
    "* *中立*：所有其他情况。\n",
    "\n",
    "自然语言推理也被称为识别文本蕴含任务。\n",
    "例如，以下配对将被标记为 *蕴含*，因为假设中的“表达感情”可以从前提中的“拥抱彼此”推导出来。\n",
    "\n",
    "> 前提：两个女人正在拥抱彼此。\n",
    "\n",
    "> 假设：两个女人正在表达感情。\n",
    "\n",
    "以下是 *矛盾* 的例子，因为“运行代码示例”表明“没有睡觉”而不是“正在睡觉”。\n",
    "\n",
    "> 前提：一个男人正在运行《深入深度学习》中的代码示例。\n",
    "\n",
    "> 假设：这个男人正在睡觉。\n",
    "\n",
    "第三个例子展示了 *中立* 关系，因为从“正在为我们表演”这一事实中既不能推导出“著名”也不能推导出“不著名”。\n",
    "\n",
    "> 前提：音乐家们正在为我们表演。\n",
    "\n",
    "> 假设：这些音乐家很著名。\n",
    "\n",
    "自然语言推理一直是理解自然语言的核心话题。\n",
    "它的应用范围广泛，从信息检索到开放领域问答。\n",
    "为了研究这个问题，我们将从调查一个流行的自然语言推理基准数据集开始。\n",
    "\n",
    "## 斯坦福自然语言推理（SNLI）数据集\n",
    "\n",
    "[**斯坦福自然语言推理（SNLI）语料库**] 是一个包含超过500,000条带标签的英语句子对的数据集 :cite:`Bowman.Angeli.Potts.ea.2015`。\n",
    "我们下载并将提取的SNLI数据集存储在路径 `../data/snli_1.0` 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a77a4ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:26:21.484092Z",
     "iopub.status.busy": "2023-08-18T19:26:21.483477Z",
     "iopub.status.idle": "2023-08-18T19:26:29.377743Z",
     "shell.execute_reply": "2023-08-18T19:26:29.376828Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ../data/snli_1.0.zip from https://nlp.stanford.edu/projects/snli/snli_1.0.zip...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "#@save\n",
    "d2l.DATA_HUB['SNLI'] = (\n",
    "    'https://nlp.stanford.edu/projects/snli/snli_1.0.zip',\n",
    "    '9fcde07509c7e87ec61c640c1b2753d9041758e4')\n",
    "\n",
    "data_dir = d2l.download_extract('SNLI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98299766",
   "metadata": {
    "origin_pos": 3
   },
   "source": [
    "### [**读取数据集**]\n",
    "\n",
    "原始的SNLI数据集包含的信息比我们实验中实际需要的要丰富得多。因此，我们定义了一个函数`read_snli`来仅提取数据集的一部分，然后返回前提、假设及其标签的列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15051d2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:26:29.381653Z",
     "iopub.status.busy": "2023-08-18T19:26:29.381256Z",
     "iopub.status.idle": "2023-08-18T19:26:29.388949Z",
     "shell.execute_reply": "2023-08-18T19:26:29.388184Z"
    },
    "origin_pos": 4,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def read_snli(data_dir, is_train):\n",
    "    \"\"\"Read the SNLI dataset into premises, hypotheses, and labels.\"\"\"\n",
    "    def extract_text(s):\n",
    "        # Remove information that will not be used by us\n",
    "        s = re.sub('\\\\(', '', s)\n",
    "        s = re.sub('\\\\)', '', s)\n",
    "        # Substitute two or more consecutive whitespace with space\n",
    "        s = re.sub('\\\\s{2,}', ' ', s)\n",
    "        return s.strip()\n",
    "    label_set = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt'\n",
    "                             if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "    premises = [extract_text(row[1]) for row in rows if row[0] in label_set]\n",
    "    hypotheses = [extract_text(row[2]) for row in rows if row[0] in label_set]\n",
    "    labels = [label_set[row[0]] for row in rows if row[0] in label_set]\n",
    "    return premises, hypotheses, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5708690",
   "metadata": {
    "origin_pos": 5
   },
   "source": [
    "现在让我们[**打印前3对**]前提和假设，以及它们的标签（\"0\"、\"1\"和\"2\"分别对应\"蕴含\"、\"矛盾\"和\"中立\"）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c2e177a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:26:29.392028Z",
     "iopub.status.busy": "2023-08-18T19:26:29.391753Z",
     "iopub.status.idle": "2023-08-18T19:26:41.816256Z",
     "shell.execute_reply": "2023-08-18T19:26:41.815386Z"
    },
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "premise: A person on a horse jumps over a broken down airplane .\n",
      "hypothesis: A person is training his horse for a competition .\n",
      "label: 2\n",
      "premise: A person on a horse jumps over a broken down airplane .\n",
      "hypothesis: A person is at a diner , ordering an omelette .\n",
      "label: 1\n",
      "premise: A person on a horse jumps over a broken down airplane .\n",
      "hypothesis: A person is outdoors , on a horse .\n",
      "label: 0\n"
     ]
    }
   ],
   "source": [
    "train_data = read_snli(data_dir, is_train=True)\n",
    "for x0, x1, y in zip(train_data[0][:3], train_data[1][:3], train_data[2][:3]):\n",
    "    print('premise:', x0)\n",
    "    print('hypothesis:', x1)\n",
    "    print('label:', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02260619",
   "metadata": {
    "origin_pos": 7
   },
   "source": [
    "训练集大约有550000对，\n",
    "测试集大约有10000对。\n",
    "以下显示\n",
    "三个标签“entailment”、“contradiction”和“neutral”在\n",
    "训练集和测试集中都是平衡的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3399eb6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:26:41.821385Z",
     "iopub.status.busy": "2023-08-18T19:26:41.820790Z",
     "iopub.status.idle": "2023-08-18T19:26:42.248143Z",
     "shell.execute_reply": "2023-08-18T19:26:42.247216Z"
    },
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[183416, 183187, 182764]\n",
      "[3368, 3237, 3219]\n"
     ]
    }
   ],
   "source": [
    "test_data = read_snli(data_dir, is_train=False)\n",
    "for data in [train_data, test_data]:\n",
    "    print([[row for row in data[2]].count(i) for i in range(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0daebd",
   "metadata": {
    "origin_pos": 9
   },
   "source": [
    "### [**定义用于加载数据集的类**]\n",
    "\n",
    "下面通过继承Gluon中的`Dataset`类来定义一个用于加载SNLI数据集的类。类构造函数中的参数`num_steps`指定了文本序列的长度，以便每个小批量的序列具有相同的形状。\n",
    "换句话说，\n",
    "较长序列中超过前`num_steps`个之后的标记将被裁剪，而较短的序列将附加特殊标记“&lt;pad&gt;”，直到其长度达到`num_steps`。\n",
    "通过实现`__getitem__`函数，我们可以任意访问索引`idx`对应的前提、假设和标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3dd55aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:26:42.251762Z",
     "iopub.status.busy": "2023-08-18T19:26:42.251121Z",
     "iopub.status.idle": "2023-08-18T19:26:42.259106Z",
     "shell.execute_reply": "2023-08-18T19:26:42.258283Z"
    },
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "class SNLIDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"A customized dataset to load the SNLI dataset.\"\"\"\n",
    "    def __init__(self, dataset, num_steps, vocab=None):\n",
    "        self.num_steps = num_steps\n",
    "        all_premise_tokens = d2l.tokenize(dataset[0])\n",
    "        all_hypothesis_tokens = d2l.tokenize(dataset[1])\n",
    "        if vocab is None:\n",
    "            self.vocab = d2l.Vocab(all_premise_tokens + all_hypothesis_tokens,\n",
    "                                   min_freq=5, reserved_tokens=['<pad>'])\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "        self.premises = self._pad(all_premise_tokens)\n",
    "        self.hypotheses = self._pad(all_hypothesis_tokens)\n",
    "        self.labels = torch.tensor(dataset[2])\n",
    "        print('read ' + str(len(self.premises)) + ' examples')\n",
    "\n",
    "    def _pad(self, lines):\n",
    "        return torch.tensor([d2l.truncate_pad(\n",
    "            self.vocab[line], self.num_steps, self.vocab['<pad>'])\n",
    "                         for line in lines])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.premises[idx], self.hypotheses[idx]), self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.premises)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99d881e",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "### [**整合所有内容**]\n",
    "\n",
    "现在我们可以调用`read_snli`函数和`SNLIDataset`类来下载SNLI数据集，并返回训练集和测试集的`DataLoader`实例，以及训练集的词汇表。\n",
    "值得注意的是，我们必须使用从训练集中构建的词汇表\n",
    "作为测试集的词汇表。\n",
    "因此，测试集中出现的任何新词对于在训练集上训练的模型来说都是未知的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af25e511",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:26:42.262443Z",
     "iopub.status.busy": "2023-08-18T19:26:42.261877Z",
     "iopub.status.idle": "2023-08-18T19:26:42.267478Z",
     "shell.execute_reply": "2023-08-18T19:26:42.266590Z"
    },
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def load_data_snli(batch_size, num_steps=50):\n",
    "    \"\"\"Download the SNLI dataset and return data iterators and vocabulary.\"\"\"\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "    data_dir = d2l.download_extract('SNLI')\n",
    "    train_data = read_snli(data_dir, True)\n",
    "    test_data = read_snli(data_dir, False)\n",
    "    train_set = SNLIDataset(train_data, num_steps)\n",
    "    test_set = SNLIDataset(test_data, num_steps, train_set.vocab)\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=num_workers)\n",
    "    test_iter = torch.utils.data.DataLoader(test_set, batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=num_workers)\n",
    "    return train_iter, test_iter, train_set.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52342d52",
   "metadata": {
    "origin_pos": 15
   },
   "source": [
    "在这里我们将批量大小设置为128，序列长度设置为50，\n",
    "然后调用`load_data_snli`函数来获取数据迭代器和词汇表。\n",
    "接着我们打印词汇表的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16d10217",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:26:42.270825Z",
     "iopub.status.busy": "2023-08-18T19:26:42.270189Z",
     "iopub.status.idle": "2023-08-18T19:27:26.541090Z",
     "shell.execute_reply": "2023-08-18T19:27:26.540150Z"
    },
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 549367 examples\n",
      "read 9824 examples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18678"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter, test_iter, vocab = load_data_snli(128, 50)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fba2f1",
   "metadata": {
    "origin_pos": 17
   },
   "source": [
    "现在我们打印第一个小批量的形状。\n",
    "与情感分析不同，\n",
    "我们有两个输入 `X[0]` 和 `X[1]` 代表前提和假设的配对。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3cf1e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:27:26.544797Z",
     "iopub.status.busy": "2023-08-18T19:27:26.544188Z",
     "iopub.status.idle": "2023-08-18T19:27:26.884432Z",
     "shell.execute_reply": "2023-08-18T19:27:26.883416Z"
    },
    "origin_pos": 18,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 50])\n",
      "torch.Size([128, 50])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "for X, Y in train_iter:\n",
    "    print(X[0].shape)\n",
    "    print(X[1].shape)\n",
    "    print(Y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85258a9e",
   "metadata": {
    "origin_pos": 19
   },
   "source": [
    "## 摘要\n",
    "\n",
    "* 自然语言推理研究从前提中能否推导出假设，其中前提是文本序列。\n",
    "* 在自然语言推理中，前提和假设之间的关系包括蕴含、矛盾和中立。\n",
    "* 斯坦福自然语言推理（SNLI）语料库是自然语言推理的一个流行基准数据集。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 机器翻译长期以来都是基于输出翻译与真实翻译之间的表面$n$-gram匹配来进行评估的。你能设计一种使用自然语言推理来评估机器翻译结果的方法吗？\n",
    "1. 我们如何调整超参数以减少词汇量？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f98055e",
   "metadata": {
    "origin_pos": 21,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[讨论](https://discuss.d2l.ai/t/1388)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-en",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}