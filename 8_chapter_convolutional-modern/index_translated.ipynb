{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c055ef3",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 现代卷积神经网络\n",
    ":label:`chap_modern_cnn`\n",
    "\n",
    "现在我们已经理解了连接CNN的基本知识，让我们来了解一下现代的CNN架构。由于不断有令人兴奋的新设计被添加进来，这次巡览必然是不完整的。它们的重要性不仅在于可以直接用于视觉任务，还在于它们可以作为更高级任务（如跟踪:cite:`Zhang.Sun.Jiang.ea.2021`、分割:cite:`Long.Shelhamer.Darrell.2015`、物体检测:cite:`Redmon.Farhadi.2018`或风格转换:cite:`Gatys.Ecker.Bethge.2016`）的基础特征生成器。在本章中，大多数章节都对应着一个重要的CNN架构，这些架构在某个时期或当前是许多研究项目和部署系统构建的基础模型。每一个这样的网络都曾短暂地成为主导架构，并且许多都在[ImageNet竞赛](https://www.image-net.org/challenges/LSVRC/)中获得过冠军或亚军，自2010年以来该竞赛一直是计算机视觉监督学习进展的晴雨表。直到最近，Transformers才开始取代CNN，始于:citet:`Dosovitskiy.Beyer.Kolesnikov.ea.2021`，随后是Swin Transformer :cite:`liu2021swin`。我们将在:numref:`chap_attention-and-transformers`中讨论这一发展。\n",
    "\n",
    "虽然*深度*神经网络的想法相当简单（堆叠多层），但不同架构和超参数选择下的性能可能会有很大差异。本章描述的神经网络是直觉、一些数学见解以及大量试验和错误的结果。我们将按照时间顺序介绍这些模型，部分原因是为了传达一种历史感，这样你可以自己形成对领域发展方向的直觉，甚至可能开发出自己的架构。例如，本章中描述的批量归一化和残差连接提供了训练和设计深层模型的两个流行想法，这两个想法此后也被应用于超出计算机视觉之外的架构中。\n",
    "\n",
    "我们从AlexNet :cite:`Krizhevsky.Sutskever.Hinton.2012`开始现代CNN之旅，这是第一个大规模部署以在大型视觉挑战中击败传统计算机视觉方法的网络；VGG网络:cite:`Simonyan.Zisserman.2014`，它使用了许多重复元素块；网络中的网络(NiN)，它将整个神经网络逐片卷积输入:cite:`Lin.Chen.Yan.2013`；GoogLeNet，使用具有多分支卷积的网络:cite:`Szegedy.Liu.Jia.ea.2015`；残差网络(ResNet) :cite:`He.Zhang.Ren.ea.2016`，至今仍是计算机视觉中最受欢迎的现成架构之一；ResNeXt块 :cite:`Xie.Girshick.Dollar.ea.2017` 用于稀疏连接；以及DenseNet :cite:`Huang.Liu.Van-Der-Maaten.ea.2017` 作为残差架构的泛化。随着时间推移，许多针对高效网络的特殊优化已被开发出来，例如坐标偏移(ShiftNet) :cite:`wu2018shift`。这最终导致了自动搜索高效的架构，如MobileNet v3 :cite:`Howard.Sandler.Chu.ea.2019`。这也包括了半自动化设计探索 :citet:`Radosavovic.Kosaraju.Girshick.ea.2020`，这导致了RegNetX/Y，我们将在本章后面讨论。这项工作之所以有指导意义，是因为它提供了一条结合蛮力计算与实验者聪明才智来寻找高效设计空间的路径。值得注意的是 :citet:`liu2022convnet` 的工作，因为它表明训练技术（例如优化器、数据增强和正则化）在提高准确性方面起着关键作用。它还表明，随着计算能力和数据量的增长，一些长期持有的假设（如卷积窗口的大小）可能需要重新审视。我们将在本章后续内容中讨论这些问题及其他更多问题。\n",
    "\n",
    ":begin_tab:toc\n",
    " - [alexnet](alexnet.ipynb)\n",
    " - [vgg](vgg.ipynb)\n",
    " - [nin](nin.ipynb)\n",
    " - [googlenet](googlenet.ipynb)\n",
    " - [batch-norm](batch-norm.ipynb)\n",
    " - [resnet](resnet.ipynb)\n",
    " - [densenet](densenet.ipynb)\n",
    " - [cnn-design](cnn-design.ipynb)\n",
    ":end_tab:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}