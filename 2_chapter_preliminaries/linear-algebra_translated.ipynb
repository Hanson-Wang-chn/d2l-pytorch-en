{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a622c08e",
   "metadata": {
    "origin_pos": 1
   },
   "source": [
    "# 线性代数\n",
    ":label:`sec_linear-algebra`\n",
    "\n",
    "到目前为止，我们可以将数据集加载到张量中，并使用基本的数学运算来操作这些张量。为了开始构建复杂的模型，我们还需要一些线性代数的工具。本节将温和地介绍最基础的概念，从标量算术开始，逐步提升到矩阵乘法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc36b473",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:51.529162Z",
     "iopub.status.busy": "2023-08-18T19:41:51.528467Z",
     "iopub.status.idle": "2023-08-18T19:41:53.438267Z",
     "shell.execute_reply": "2023-08-18T19:41:53.437059Z"
    },
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a38674",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "## 标量\n",
    "\n",
    "大多数日常数学\n",
    "涉及一次处理一个\n",
    "数字。正式地，我们称这些值为*标量*。\n",
    "例如，帕洛阿尔托的温度\n",
    "是宜人的72华氏度。\n",
    "如果你想将温度转换为摄氏度，\n",
    "你会计算表达式\n",
    "$c = \\frac{5}{9}(f - 32)$，设$f$为72。\n",
    "在这个等式中，数值\n",
    "5、9和32是常数标量。\n",
    "变量$c$和$f$\n",
    "通常代表未知标量。\n",
    "\n",
    "我们用普通的小写字母\n",
    "表示标量\n",
    "（如$x$、$y$和$z$）\n",
    "并用$\\mathbb{R}$表示所有（连续的）\n",
    "*实数值*标量的空间。\n",
    "为了方便起见，我们将跳过\n",
    "对*空间*的严格定义：\n",
    "只需记住表达式$x \\in \\mathbb{R}$\n",
    "是一种正式的说法，表示$x$是一个实数值标量。\n",
    "符号$\\in$（发音为“属于”）\n",
    "表示集合中的成员。\n",
    "例如，$x, y \\in \\{0, 1\\}$\n",
    "表示$x$和$y$是只能取值0或1的变量。\n",
    "\n",
    "(**标量被实现为仅包含一个元素的张量。**)\n",
    "下面，我们分配两个标量\n",
    "并执行熟悉的加法、乘法、\n",
    "除法和指数运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fc9ba1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:53.442690Z",
     "iopub.status.busy": "2023-08-18T19:41:53.442040Z",
     "iopub.status.idle": "2023-08-18T19:41:53.472277Z",
     "shell.execute_reply": "2023-08-18T19:41:53.471491Z"
    },
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "x + y, x * y, x / y, x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b20517b",
   "metadata": {
    "origin_pos": 11
   },
   "source": [
    "## 向量\n",
    "\n",
    "就当前目的而言，[**你可以将向量视为固定长度的标量数组。**] 与它们在代码中的对应物一样，我们称这些标量为向量的*元素*（同义词包括*条目*和*分量*）。当向量代表来自现实世界数据集的实例时，它们的值具有某种现实意义。例如，如果我们正在训练一个模型来预测贷款违约的风险，我们可能会将每个申请人与一个向量关联起来，该向量的分量对应于像他们的收入、就业年限或先前违约次数这样的数量。如果我们正在研究心脏病发作的风险，每个向量可能代表一名患者，其分量可能对应于他们最近的生命体征、胆固醇水平、每天锻炼分钟数等。我们用粗体小写字母表示向量（例如，$\\mathbf{x}$, $\\mathbf{y}$, 和 $\\mathbf{z}$）。\n",
    "\n",
    "向量被实现为$1^{\\textrm{st}}$-阶张量。一般来说，这种张量可以有任意长度，受内存限制。注意：在Python中，就像大多数编程语言一样，向量索引从$0$开始，也称为*零基索引*，而在线性代数中下标从$1$开始（一基索引）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91cd966f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:53.475759Z",
     "iopub.status.busy": "2023-08-18T19:41:53.475141Z",
     "iopub.status.idle": "2023-08-18T19:41:53.481106Z",
     "shell.execute_reply": "2023-08-18T19:41:53.479872Z"
    },
    "origin_pos": 13,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(3)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a603956",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "我们可以使用下标来引用向量中的元素。例如，$x_2$ 表示 $\\mathbf{x}$ 的第二个元素。由于 $x_2$ 是一个标量，我们不会将其加粗。默认情况下，我们通过将向量的元素垂直堆叠来可视化向量。\n",
    "\n",
    "$$\\mathbf{x} =\\begin{bmatrix}x_{1}  \\\\ \\vdots  \\\\x_{n}\\end{bmatrix},$$\n",
    ":eqlabel:`eq_vec_def`\n",
    "\n",
    "这里 $x_1, \\ldots, x_n$ 是向量的元素。稍后，我们将区分这种*列向量*和*行向量*，行向量的元素是水平堆叠的。回想一下，[**我们通过索引来访问张量的元素。**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba15a197",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:53.485066Z",
     "iopub.status.busy": "2023-08-18T19:41:53.484260Z",
     "iopub.status.idle": "2023-08-18T19:41:53.492710Z",
     "shell.execute_reply": "2023-08-18T19:41:53.491415Z"
    },
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a04823",
   "metadata": {
    "origin_pos": 18
   },
   "source": [
    "为了表示一个向量包含 $n$ 个元素，我们写成 $\\mathbf{x} \\in \\mathbb{R}^n$。形式上，我们称 $n$ 为向量的*维度*。[**在代码中，这对应于张量的长度**]，可以通过 Python 的内置 `len` 函数访问。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "871cd7e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:53.497529Z",
     "iopub.status.busy": "2023-08-18T19:41:53.496794Z",
     "iopub.status.idle": "2023-08-18T19:41:53.502332Z",
     "shell.execute_reply": "2023-08-18T19:41:53.501510Z"
    },
    "origin_pos": 19,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e190d69",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "我们也可以通过`shape`属性访问长度。\n",
    "形状是一个元组，表示张量在每个轴上的长度。\n",
    "（**只有一个轴的张量的形状只有一个元素。**）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34ea04c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:53.505748Z",
     "iopub.status.busy": "2023-08-18T19:41:53.505180Z",
     "iopub.status.idle": "2023-08-18T19:41:53.510136Z",
     "shell.execute_reply": "2023-08-18T19:41:53.509337Z"
    },
    "origin_pos": 21,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e416733f",
   "metadata": {
    "origin_pos": 22
   },
   "source": [
    "通常，“维度”这个词被过度使用，既表示轴的数量，也表示特定轴的长度。为了避免这种混淆，我们用*阶数*来指代轴的数量，并且仅用*维数*来指代分量的数量。\n",
    "\n",
    "## 矩阵\n",
    "\n",
    "正如标量是$0^{\\textrm{th}}$阶张量，向量是$1^{\\textrm{st}}$阶张量一样，矩阵是$2^{\\textrm{nd}}$阶张量。我们用粗体大写字母（例如$\\mathbf{X}$, $\\mathbf{Y}$和$\\mathbf{Z}$）表示矩阵，并在代码中通过具有两个轴的张量来表示它们。表达式$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$表示矩阵$\\mathbf{A}$包含$m \\times n$个实数值标量，排列成$m$行和$n$列。当$m = n$时，我们说这个矩阵是*方阵*。直观上，我们可以将任何矩阵描绘为一个表格。要引用单个元素，我们需要同时下标行索引和列索引，例如，$a_{ij}$是属于$\\mathbf{A}$的第$i^{\\textrm{th}}$行和第$j^{\\textrm{th}}$列的值：\n",
    "\n",
    "$$\\mathbf{A}=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix}.$$\n",
    ":eqlabel:`eq_matrix_def`\n",
    "\n",
    "\n",
    "在代码中，我们通过形状为($m$, $n$)的$2^{\\textrm{nd}}$阶张量来表示矩阵$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$。\n",
    "[**我们可以通过将所需的形状传递给`reshape`函数，将任何适当大小的$m \\times n$张量转换为$m \\times n$矩阵**]："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80c49751",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:53.513569Z",
     "iopub.status.busy": "2023-08-18T19:41:53.512931Z",
     "iopub.status.idle": "2023-08-18T19:41:53.518545Z",
     "shell.execute_reply": "2023-08-18T19:41:53.517740Z"
    },
    "origin_pos": 24,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6).reshape(3, 2)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d90da51",
   "metadata": {
    "origin_pos": 27
   },
   "source": [
    "有时我们想翻转坐标轴。\n",
    "当我们交换矩阵的行和列时，\n",
    "结果称为其*转置*。\n",
    "形式上，我们用$\\mathbf{A}^\\top$表示矩阵$\\mathbf{A}$的转置，\n",
    "如果$\\mathbf{B} = \\mathbf{A}^\\top$，那么对于所有的$i$和$j$有$b_{ij} = a_{ji}$。\n",
    "因此，一个$m \\times n$矩阵的转置\n",
    "是一个$n \\times m$矩阵：\n",
    "\n",
    "$$\n",
    "\\mathbf{A}^\\top =\n",
    "\\begin{bmatrix}\n",
    "    a_{11} & a_{21} & \\dots  & a_{m1} \\\\\n",
    "    a_{12} & a_{22} & \\dots  & a_{m2} \\\\\n",
    "    \\vdots & \\vdots & \\ddots  & \\vdots \\\\\n",
    "    a_{1n} & a_{2n} & \\dots  & a_{mn}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "在代码中，我们可以如下访问任何（**矩阵的转置**）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ef1e23b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:53.521874Z",
     "iopub.status.busy": "2023-08-18T19:41:53.521332Z",
     "iopub.status.idle": "2023-08-18T19:41:53.526566Z",
     "shell.execute_reply": "2023-08-18T19:41:53.525812Z"
    },
    "origin_pos": 28,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2, 4],\n",
       "        [1, 3, 5]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce337753",
   "metadata": {
    "origin_pos": 30
   },
   "source": [
    "[**对称矩阵是方阵的一个子集，它们等于自己的转置：$\\mathbf{A} = \\mathbf{A}^\\top$。**]\n",
    "以下矩阵是对称的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "028e06ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:53.529939Z",
     "iopub.status.busy": "2023-08-18T19:41:53.529400Z",
     "iopub.status.idle": "2023-08-18T19:41:53.535337Z",
     "shell.execute_reply": "2023-08-18T19:41:53.534568Z"
    },
    "origin_pos": 32,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "A == A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f945d82",
   "metadata": {
    "origin_pos": 35
   },
   "source": [
    "矩阵对于表示数据集非常有用。\n",
    "通常，行对应于单个记录，\n",
    "而列对应于不同的属性。\n",
    "\n",
    "## 张量\n",
    "\n",
    "虽然仅使用标量、向量和矩阵你可以在机器学习之旅中走得相当远，\n",
    "但最终你可能需要处理更高阶的[**张量**]。\n",
    "张量（**为我们提供了一种描述扩展到$n^{\\textrm{th}}$阶数组的通用方式。**）\n",
    "我们之所以称软件对象中的*张量类*为“张量”，\n",
    "正是因为它们也可以有任意数量的轴。\n",
    "尽管将*张量*这个词同时用于数学对象及其代码实现可能会让人感到困惑，\n",
    "但我们的意思通常可以从上下文中清楚地看出。\n",
    "我们用一种特殊字体的大写字母来表示一般张量\n",
    "（例如，$\\mathsf{X}$, $\\mathsf{Y}$, 和 $\\mathsf{Z}$），\n",
    "并且它们的索引机制\n",
    "（例如，$x_{ijk}$ 和 $[\\mathsf{X}]_{1, 2i-1, 3}$）\n",
    "自然地继承自矩阵。\n",
    "\n",
    "当我们开始处理图像时，张量将变得更加重要。\n",
    "每张图像都以一个$3^{\\textrm{rd}}$阶张量的形式出现，\n",
    "其轴分别对应高度、宽度和*通道*。\n",
    "在每个空间位置上，每种颜色（红色、绿色和蓝色）的强度\n",
    "沿着通道堆叠。\n",
    "此外，一系列图像在代码中由一个$4^{\\textrm{th}}$阶张量表示，\n",
    "其中不同的图像沿第一个轴索引。\n",
    "如同向量和矩阵一样，通过增加形状分量的数量来构建高阶张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "306d610e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:53.538891Z",
     "iopub.status.busy": "2023-08-18T19:41:53.538210Z",
     "iopub.status.idle": "2023-08-18T19:41:53.546164Z",
     "shell.execute_reply": "2023-08-18T19:41:53.545027Z"
    },
    "origin_pos": 37,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(24).reshape(2, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3113bae4",
   "metadata": {
    "origin_pos": 40
   },
   "source": [
    "## 张量算术的基本属性\n",
    "\n",
    "标量、向量、矩阵，\n",
    "以及高阶张量\n",
    "都具有一些方便的属性。\n",
    "例如，逐元素操作\n",
    "产生的输出形状与操作数\n",
    "相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53a34bc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:53.550917Z",
     "iopub.status.busy": "2023-08-18T19:41:53.550017Z",
     "iopub.status.idle": "2023-08-18T19:41:53.558241Z",
     "shell.execute_reply": "2023-08-18T19:41:53.557366Z"
    },
    "origin_pos": 42,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " tensor([[ 0.,  2.,  4.],\n",
       "         [ 6.,  8., 10.]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "B = A.clone()  # Assign a copy of A to B by allocating new memory\n",
    "A, A + B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7d4d8e",
   "metadata": {
    "origin_pos": 45
   },
   "source": [
    "两个矩阵的逐元素乘积称为它们的*Hadamard乘积*（记作$\\odot$）。我们可以写出两个矩阵$\\mathbf{A}, \\mathbf{B} \\in \\mathbb{R}^{m \\times n}$的Hadamard乘积的各个元素：\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\odot \\mathbf{B} =\n",
    "\\begin{bmatrix}\n",
    "    a_{11}  b_{11} & a_{12}  b_{12} & \\dots  & a_{1n}  b_{1n} \\\\\n",
    "    a_{21}  b_{21} & a_{22}  b_{22} & \\dots  & a_{2n}  b_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_{m1}  b_{m1} & a_{m2}  b_{m2} & \\dots  & a_{mn}  b_{mn}\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e2c0645",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:53.561758Z",
     "iopub.status.busy": "2023-08-18T19:41:53.561198Z",
     "iopub.status.idle": "2023-08-18T19:41:53.567597Z",
     "shell.execute_reply": "2023-08-18T19:41:53.566714Z"
    },
    "origin_pos": 46,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  4.],\n",
       "        [ 9., 16., 25.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e782293c",
   "metadata": {
    "origin_pos": 47
   },
   "source": [
    "[**标量与张量相加或相乘**]会产生一个与原张量形状相同的结果。这里，张量的每个元素都与该标量相加（或相乘）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5c86fb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:53.571034Z",
     "iopub.status.busy": "2023-08-18T19:41:53.570428Z",
     "iopub.status.idle": "2023-08-18T19:41:53.577301Z",
     "shell.execute_reply": "2023-08-18T19:41:53.576396Z"
    },
    "origin_pos": 49,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2,  3,  4,  5],\n",
       "          [ 6,  7,  8,  9],\n",
       "          [10, 11, 12, 13]],\n",
       " \n",
       "         [[14, 15, 16, 17],\n",
       "          [18, 19, 20, 21],\n",
       "          [22, 23, 24, 25]]]),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "a + X, (a * X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bc0c1b",
   "metadata": {
    "origin_pos": 52
   },
   "source": [
    "## 降维\n",
    ":label:`subsec_lin-alg-reduction`\n",
    "\n",
    "通常，我们希望计算[**张量元素的总和。**]\n",
    "为了表示长度为 $n$ 的向量 $\\mathbf{x}$ 的元素总和，我们写作 $\\sum_{i=1}^n x_i$。有一个简单的函数可以实现这一点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a3c4cbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:53.580908Z",
     "iopub.status.busy": "2023-08-18T19:41:53.580306Z",
     "iopub.status.idle": "2023-08-18T19:41:53.588497Z",
     "shell.execute_reply": "2023-08-18T19:41:53.587623Z"
    },
    "origin_pos": 54,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2.]), tensor(3.))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(3, dtype=torch.float32)\n",
    "x, x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e031e9",
   "metadata": {
    "origin_pos": 57
   },
   "source": [
    "为了表示[**对任意形状张量的元素求和**]，我们只需对其所有轴进行求和。例如，一个 $m \\times n$ 矩阵 $\\mathbf{A}$ 的元素之和可以写作 $\\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7594e574",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:53.593587Z",
     "iopub.status.busy": "2023-08-18T19:41:53.592920Z",
     "iopub.status.idle": "2023-08-18T19:41:53.602105Z",
     "shell.execute_reply": "2023-08-18T19:41:53.600812Z"
    },
    "origin_pos": 58,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), tensor(15.))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, A.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9a0995",
   "metadata": {
    "origin_pos": 60
   },
   "source": [
    "默认情况下，调用 sum 函数会沿着张量的所有轴*减少*张量，最终生成一个标量。我们的库也允许我们[**指定应沿着哪些轴减少张量。**]要沿着行（轴 0）对所有元素求和，我们在 `sum` 中指定 `axis=0`。由于输入矩阵沿着轴 0 减少以生成输出向量，此轴在输出的形状中缺失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14d191be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:53.606330Z",
     "iopub.status.busy": "2023-08-18T19:41:53.605594Z",
     "iopub.status.idle": "2023-08-18T19:41:53.612393Z",
     "shell.execute_reply": "2023-08-18T19:41:53.611227Z"
    },
    "origin_pos": 61,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.Size([3]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, A.sum(axis=0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f9f461",
   "metadata": {
    "origin_pos": 63
   },
   "source": [
    "指定 `axis=1` 将通过累加所有列的元素来减少列维度（轴 1）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee3c0559",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:53.615938Z",
     "iopub.status.busy": "2023-08-18T19:41:53.615181Z",
     "iopub.status.idle": "2023-08-18T19:41:53.621919Z",
     "shell.execute_reply": "2023-08-18T19:41:53.620799Z"
    },
    "origin_pos": 64,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.Size([2]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, A.sum(axis=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40d7419",
   "metadata": {
    "origin_pos": 66
   },
   "source": [
    "通过行和列的求和来减少矩阵等同于对矩阵的所有元素进行求和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25b99ea4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:53.625271Z",
     "iopub.status.busy": "2023-08-18T19:41:53.624779Z",
     "iopub.status.idle": "2023-08-18T19:41:53.631897Z",
     "shell.execute_reply": "2023-08-18T19:41:53.630699Z"
    },
    "origin_pos": 67,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=[0, 1]) == A.sum()  # Same as A.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46224eef",
   "metadata": {
    "origin_pos": 69
   },
   "source": [
    "[**一个相关的量是*均值*，也称为*平均数*。**]\n",
    "我们通过将总和除以元素总数来计算均值。\n",
    "因为计算均值非常常见，\n",
    "它有一个专门的库函数，\n",
    "其工作方式类似于`sum`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f41e037",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:53.635407Z",
     "iopub.status.busy": "2023-08-18T19:41:53.634799Z",
     "iopub.status.idle": "2023-08-18T19:41:53.642980Z",
     "shell.execute_reply": "2023-08-18T19:41:53.641833Z"
    },
    "origin_pos": 71,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.5000), tensor(2.5000))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(), A.sum() / A.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c73c2c9",
   "metadata": {
    "origin_pos": 73
   },
   "source": [
    "同样，用于计算平均值的函数也可以沿着特定轴减少张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f268d8be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:53.646514Z",
     "iopub.status.busy": "2023-08-18T19:41:53.645792Z",
     "iopub.status.idle": "2023-08-18T19:41:53.653946Z",
     "shell.execute_reply": "2023-08-18T19:41:53.652643Z"
    },
    "origin_pos": 74,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.5000, 2.5000, 3.5000]), tensor([1.5000, 2.5000, 3.5000]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(axis=0), A.sum(axis=0) / A.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272a70c1",
   "metadata": {
    "origin_pos": 76
   },
   "source": [
    "## 非缩减求和\n",
    ":label:`subsec_lin-alg-non-reduction`\n",
    "\n",
    "有时在调用求和或均值函数时[**保持轴的数量不变**]会很有用。\n",
    "当我们想要使用广播机制时，这一点很重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "863c5aca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:53.658423Z",
     "iopub.status.busy": "2023-08-18T19:41:53.658026Z",
     "iopub.status.idle": "2023-08-18T19:41:53.666522Z",
     "shell.execute_reply": "2023-08-18T19:41:53.665397Z"
    },
    "origin_pos": 77,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.],\n",
      "        [12.]]) torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "print(sum_A, sum_A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7e1c95",
   "metadata": {
    "origin_pos": 79
   },
   "source": [
    "例如，由于`sum_A`在对每一行求和后仍保留其两个轴，\n",
    "我们可以（**通过广播将`A`除以`sum_A`**）\n",
    "来创建一个每行总和为$1$的矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93d20f26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:53.669896Z",
     "iopub.status.busy": "2023-08-18T19:41:53.669623Z",
     "iopub.status.idle": "2023-08-18T19:41:53.675548Z",
     "shell.execute_reply": "2023-08-18T19:41:53.674706Z"
    },
    "origin_pos": 80,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.3333, 0.6667],\n",
       "        [0.2500, 0.3333, 0.4167]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A / sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d208f582",
   "metadata": {
    "origin_pos": 81
   },
   "source": [
    "如果我们想要计算[**A沿着某个轴的元素累积和**]，比如说`axis=0`（逐行），我们可以调用`cumsum`函数。按照设计，这个函数不会沿着任何轴减少输入张量的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2de0ae7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:53.678804Z",
     "iopub.status.busy": "2023-08-18T19:41:53.678536Z",
     "iopub.status.idle": "2023-08-18T19:41:53.684437Z",
     "shell.execute_reply": "2023-08-18T19:41:53.683619Z"
    },
    "origin_pos": 82,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 5., 7.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.cumsum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f6a397",
   "metadata": {
    "origin_pos": 84
   },
   "source": [
    "## 点积\n",
    "\n",
    "到目前为止，我们只执行了逐元素操作、求和以及平均。\n",
    "如果仅限于此，线性代数\n",
    "就不值得单独作为一个章节来讨论了。\n",
    "幸运的是，从这里开始事情变得更有趣了。\n",
    "最基本的操作之一是点积。\n",
    "给定两个向量 $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d$，\n",
    "它们的*点积* $\\mathbf{x}^\\top \\mathbf{y}$（也称为*内积*，$\\langle \\mathbf{x}, \\mathbf{y}  \\rangle$）\n",
    "是对相同位置元素乘积的求和：\n",
    "$\\mathbf{x}^\\top \\mathbf{y} = \\sum_{i=1}^{d} x_i y_i$。\n",
    "\n",
    "[~~两个向量的*点积*是对相同位置元素乘积的求和~~]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5575e11a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:53.687734Z",
     "iopub.status.busy": "2023-08-18T19:41:53.687178Z",
     "iopub.status.idle": "2023-08-18T19:41:53.696147Z",
     "shell.execute_reply": "2023-08-18T19:41:53.695369Z"
    },
    "origin_pos": 86,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2.]), tensor([1., 1., 1.]), tensor(3.))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones(3, dtype = torch.float32)\n",
    "x, y, torch.dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9329e0",
   "metadata": {
    "origin_pos": 89
   },
   "source": [
    "（**我们可以通过先进行逐元素乘法再求和来计算两个向量的点积：**）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5186254",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:53.699396Z",
     "iopub.status.busy": "2023-08-18T19:41:53.698843Z",
     "iopub.status.idle": "2023-08-18T19:41:53.704931Z",
     "shell.execute_reply": "2023-08-18T19:41:53.703664Z"
    },
    "origin_pos": 91,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe8aff3",
   "metadata": {
    "origin_pos": 94
   },
   "source": [
    "点积在各种情况下都非常有用。\n",
    "例如，给定一组值，\n",
    "用向量 $\\mathbf{x}  \\in \\mathbb{R}^n$ 表示，\n",
    "以及一组权重，用 $\\mathbf{w} \\in \\mathbb{R}^n$ 表示，\n",
    "根据权重 $\\mathbf{w}$ 的值 $\\mathbf{x}$ 的加权和\n",
    "可以表示为点积 $\\mathbf{x}^\\top \\mathbf{w}$。\n",
    "当权重是非负的\n",
    "并且总和为 $1$，即 $\\left(\\sum_{i=1}^{n} {w_i} = 1\\right)$，\n",
    "点积表达的是*加权平均*。\n",
    "将两个向量归一化到单位长度后，\n",
    "点积表示它们之间的夹角的余弦。\n",
    "在本节的后面部分，我们将正式介绍这种*长度*的概念。\n",
    "\n",
    "\n",
    "## 矩阵-向量乘积\n",
    "\n",
    "现在我们知道了如何计算点积，\n",
    "我们可以开始理解\n",
    "$m \\times n$ 矩阵 $\\mathbf{A}$ \n",
    "与 $n$ 维向量 $\\mathbf{x}$ 之间的*乘积*。\n",
    "首先，我们通过其行向量来可视化矩阵\n",
    "\n",
    "$$\\mathbf{A}=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_m \\\\\n",
    "\\end{bmatrix},$$\n",
    "\n",
    "其中每个 $\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^n$\n",
    "是表示矩阵 $\\mathbf{A}$ 的第 $i$ 行的行向量。\n",
    "\n",
    "[**矩阵-向量乘积 $\\mathbf{A}\\mathbf{x}$\n",
    "只是一个长度为 $m$ 的列向量，\n",
    "其第 $i$ 个元素是点积\n",
    "$\\mathbf{a}^\\top_i \\mathbf{x}$:**]\n",
    "\n",
    "$$\n",
    "\\mathbf{A}\\mathbf{x}\n",
    "= \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_m \\\\\n",
    "\\end{bmatrix}\\mathbf{x}\n",
    "= \\begin{bmatrix}\n",
    " \\mathbf{a}^\\top_{1} \\mathbf{x}  \\\\\n",
    " \\mathbf{a}^\\top_{2} \\mathbf{x} \\\\\n",
    "\\vdots\\\\\n",
    " \\mathbf{a}^\\top_{m} \\mathbf{x}\\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "我们可以把与矩阵 $\\mathbf{A}\\in \\mathbb{R}^{m \\times n}$\n",
    "相乘看作是一种变换，\n",
    "它将向量从 $\\mathbb{R}^{n}$ 投影到 $\\mathbb{R}^{m}$。\n",
    "这些变换非常有用。\n",
    "例如，我们可以用某些方阵的乘法来表示旋转。\n",
    "矩阵-向量乘积还描述了\n",
    "计算神经网络每一层输出时的关键计算，\n",
    "基于前一层的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3769117",
   "metadata": {
    "origin_pos": 96,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "要以代码表示矩阵-向量乘积，我们使用`mv`函数。请注意，`A`的列维度（它沿轴1的长度）必须与`x`的维度（它的长度）相同。Python有一个便捷运算符`@`，可以执行矩阵-向量和矩阵-矩阵乘积（取决于其参数）。因此我们可以写成`A@x`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a99c29a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:53.710181Z",
     "iopub.status.busy": "2023-08-18T19:41:53.709041Z",
     "iopub.status.idle": "2023-08-18T19:41:53.719479Z",
     "shell.execute_reply": "2023-08-18T19:41:53.718068Z"
    },
    "origin_pos": 99,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.Size([3]), tensor([ 5., 14.]), tensor([ 5., 14.]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, x.shape, torch.mv(A, x), A@x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e00436",
   "metadata": {
    "origin_pos": 102
   },
   "source": [
    "## 矩阵-矩阵乘法\n",
    "\n",
    "一旦你掌握了点积和矩阵-向量乘法，那么*矩阵-矩阵乘法*应该是很直接的。\n",
    "\n",
    "假设我们有两个矩阵\n",
    "$\\mathbf{A} \\in \\mathbb{R}^{n \\times k}$ \n",
    "和 $\\mathbf{B} \\in \\mathbb{R}^{k \\times m}$:\n",
    "\n",
    "$$\\mathbf{A}=\\begin{bmatrix}\n",
    " a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n",
    " a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " a_{n1} & a_{n2} & \\cdots & a_{nk} \\\\\n",
    "\\end{bmatrix},\\quad\n",
    "\\mathbf{B}=\\begin{bmatrix}\n",
    " b_{11} & b_{12} & \\cdots & b_{1m} \\\\\n",
    " b_{21} & b_{22} & \\cdots & b_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " b_{k1} & b_{k2} & \\cdots & b_{km} \\\\\n",
    "\\end{bmatrix}.$$\n",
    "\n",
    "\n",
    "设 $\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^k$ 表示\n",
    "矩阵 $\\mathbf{A}$ 的第 $i$ 行\n",
    "的行向量，设 $\\mathbf{b}_{j} \\in \\mathbb{R}^k$ 表示\n",
    "矩阵 $\\mathbf{B}$ 的第 $j$ 列\n",
    "的列向量：\n",
    "\n",
    "$$\\mathbf{A}=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_n \\\\\n",
    "\\end{bmatrix},\n",
    "\\quad \\mathbf{B}=\\begin{bmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "为了形成矩阵乘积 $\\mathbf{C} \\in \\mathbb{R}^{n \\times m}$，\n",
    "我们只需计算每个元素 $c_{ij}$\n",
    "作为 $\\mathbf{A}$ 的第 $i$ 行\n",
    "和 $\\mathbf{B}$ 的第 $j$ 列\n",
    "之间的点积，即 $\\mathbf{a}^\\top_i \\mathbf{b}_j$:\n",
    "\n",
    "$$\\mathbf{C} = \\mathbf{AB} = \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_n \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\mathbf{b}_1 & \\mathbf{a}^\\top_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^\\top_{1} \\mathbf{b}_m \\\\\n",
    " \\mathbf{a}^\\top_{2}\\mathbf{b}_1 & \\mathbf{a}^\\top_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^\\top_{2} \\mathbf{b}_m \\\\\n",
    " \\vdots & \\vdots & \\ddots &\\vdots\\\\\n",
    "\\mathbf{a}^\\top_{n} \\mathbf{b}_1 & \\mathbf{a}^\\top_{n}\\mathbf{b}_2& \\cdots& \\mathbf{a}^\\top_{n} \\mathbf{b}_m\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "[**我们可以将矩阵-矩阵乘法 $\\mathbf{AB}$\n",
    "视为执行 $m$ 次矩阵-向量乘法\n",
    "或 $m \\times n$ 次点积\n",
    "并将结果拼接在一起\n",
    "以形成一个 $n \\times m$ 的矩阵。**]\n",
    "在下面的代码片段中，\n",
    "我们在 `A` 和 `B` 上执行矩阵乘法。\n",
    "这里，`A` 是一个两行三列的矩阵，\n",
    "`B` 是一个三行四列的矩阵。\n",
    "乘法后，我们得到一个两行四列的矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99535047",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:53.723651Z",
     "iopub.status.busy": "2023-08-18T19:41:53.722762Z",
     "iopub.status.idle": "2023-08-18T19:41:53.732227Z",
     "shell.execute_reply": "2023-08-18T19:41:53.731088Z"
    },
    "origin_pos": 104,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3.,  3.,  3.,  3.],\n",
       "         [12., 12., 12., 12.]]),\n",
       " tensor([[ 3.,  3.,  3.,  3.],\n",
       "         [12., 12., 12., 12.]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.ones(3, 4)\n",
    "torch.mm(A, B), A@B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a86db3",
   "metadata": {
    "origin_pos": 107
   },
   "source": [
    "术语 *matrix--matrix multiplication* 通常简化为 *matrix multiplication*，不应与Hadamard乘积混淆。\n",
    "\n",
    "## 范数\n",
    ":label:`subsec_lin-algebra-norms`\n",
    "\n",
    "线性代数中一些最有用的运算符是*范数*。非正式地说，向量的范数告诉我们它有多大。例如，$\\ell_2$ 范数衡量向量的（欧几里得）长度。这里，我们采用的“大小”概念关注的是向量分量的幅度（而不是其维度）。\n",
    "\n",
    "范数是一个函数 $\\| \\cdot \\|$，它将一个向量映射到一个标量，并满足以下三个性质：\n",
    "\n",
    "1. 给定任何向量 $\\mathbf{x}$，如果我们通过一个标量 $\\alpha \\in \\mathbb{R}$ 对向量的所有元素进行缩放，其范数也会相应地缩放：\n",
    "   $$\\|\\alpha \\mathbf{x}\\| = |\\alpha| \\|\\mathbf{x}\\|.$$\n",
    "2. 对于任何向量 $\\mathbf{x}$ 和 $\\mathbf{y}$：范数满足三角不等式：\n",
    "   $$\\|\\mathbf{x} + \\mathbf{y}\\| \\leq \\|\\mathbf{x}\\| + \\|\\mathbf{y}\\|.$$\n",
    "3. 向量的范数是非负的，并且仅当向量为零时才消失：\n",
    "   $$\\|\\mathbf{x}\\| > 0 \\textrm{ 对所有 } \\mathbf{x} \\neq 0.$$\n",
    "\n",
    "许多函数都是有效的范数，不同的范数编码了不同的“大小”概念。我们在小学几何学中学到的计算直角三角形斜边时所用的欧几里得范数是向量元素平方和的平方根。形式上，这被称为[**$\\ell_2$ *范数***]并表示为\n",
    "\n",
    "(**$$\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}.$$**)\n",
    "\n",
    "方法 `norm` 计算 $\\ell_2$ 范数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e215c544",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:53.735890Z",
     "iopub.status.busy": "2023-08-18T19:41:53.735108Z",
     "iopub.status.idle": "2023-08-18T19:41:53.742412Z",
     "shell.execute_reply": "2023-08-18T19:41:53.741311Z"
    },
    "origin_pos": 109,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.norm(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80835a0",
   "metadata": {
    "origin_pos": 112
   },
   "source": [
    "[**$\\ell_1$ 范数**] 也很常见，与其相关的度量称为曼哈顿距离。根据定义，$\\ell_1$ 范数是向量元素绝对值的和：\n",
    "\n",
    "(**$$\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|.$$**)\n",
    "\n",
    "与 $\\ell_2$ 范数相比，它对异常值不那么敏感。为了计算 $\\ell_1$ 范数，我们将绝对值与求和操作结合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a3e0562",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:53.746107Z",
     "iopub.status.busy": "2023-08-18T19:41:53.745359Z",
     "iopub.status.idle": "2023-08-18T19:41:53.752438Z",
     "shell.execute_reply": "2023-08-18T19:41:53.751302Z"
    },
    "origin_pos": 114,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(u).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727b4843",
   "metadata": {
    "origin_pos": 117
   },
   "source": [
    "$\\ell_2$范数和$\\ell_1$范数都是更一般的$\\ell_p$*范数*的特殊情况：\n",
    "\n",
    "$$\\|\\mathbf{x}\\|_p = \\left(\\sum_{i=1}^n \\left|x_i \\right|^p \\right)^{1/p}.$$\n",
    "\n",
    "对于矩阵来说，情况更为复杂。毕竟，矩阵既可以被视为单个元素的集合，也可以被视为对向量进行操作并将其转换为其他向量的对象。例如，我们可以问矩阵-向量乘积$\\mathbf{X} \\mathbf{v}$相对于$\\mathbf{v}$可能变长多少。这种思路引出了所谓的*谱*范数。目前，我们引入[***Frobenius范数*，它计算起来要容易得多**]，其定义为矩阵元素平方和的平方根：\n",
    "\n",
    "[**$$\\|\\mathbf{X}\\|_\\textrm{F} = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}.$$**]\n",
    "\n",
    "Frobenius范数的行为就像它是矩阵形状向量的$\\ell_2$范数一样。\n",
    "调用以下函数将计算矩阵的Frobenius范数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e00a124",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:41:53.756072Z",
     "iopub.status.busy": "2023-08-18T19:41:53.755281Z",
     "iopub.status.idle": "2023-08-18T19:41:53.762237Z",
     "shell.execute_reply": "2023-08-18T19:41:53.761172Z"
    },
    "origin_pos": 119,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torch.ones((4, 9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8478a55",
   "metadata": {
    "origin_pos": 122
   },
   "source": [
    "虽然我们不想过于超前，\n",
    "但我们已经可以对这些概念为何有用建立一些直观的理解。\n",
    "在深度学习中，我们经常试图解决优化问题：\n",
    "*最大化*分配给观察数据的概率；\n",
    "*最大化*与推荐模型相关的收入；\n",
    "*最小化*预测\n",
    "与真实观察之间的距离；\n",
    "*最小化*同一人的照片的表示\n",
    "之间的距离\n",
    "同时*最大化*不同人的照片的表示\n",
    "之间的距离。\n",
    "这些距离构成了\n",
    "深度学习算法的目标，\n",
    "通常用范数来表示。\n",
    "\n",
    "\n",
    "## 讨论\n",
    "\n",
    "在这一节中，我们回顾了所有你需要了解的线性代数知识，\n",
    "以理解现代深度学习的重要部分。\n",
    "不过，线性代数还有很多内容，\n",
    "其中很多对于机器学习是有用的。\n",
    "例如，矩阵可以分解成因子，\n",
    "这些分解可以揭示现实世界数据集中的低维结构。\n",
    "机器学习的整个子领域\n",
    "专注于使用矩阵分解\n",
    "及其向高阶张量的推广\n",
    "来发现数据集中的结构并解决预测问题。\n",
    "但这本书的重点是深度学习。\n",
    "我们相信，一旦你亲自动手\n",
    "将机器学习应用于真实数据集，\n",
    "你会更倾向于学习更多的数学知识。\n",
    "因此，虽然我们保留稍后引入更多数学的权利，\n",
    "但我们在这一节就此结束。\n",
    "\n",
    "如果你渴望学习更多的线性代数，\n",
    "有很多优秀的书籍和在线资源。\n",
    "对于更高级的速成课程，可以考虑查阅\n",
    ":citet:`Strang.1993`、:citet:`Kolter.2008` 和 :citet:`Petersen.Pedersen.ea.2008`。\n",
    "\n",
    "总结：\n",
    "\n",
    "* 标量、向量、矩阵和张量是\n",
    "  线性代数中使用的基本数学对象\n",
    "  分别具有零个、一个、两个和任意数量的轴。\n",
    "* 张量可以通过索引或 `sum` 和 `mean` 等操作\n",
    "  沿指定轴进行切片或降维。\n",
    "* 元素乘积称为哈达玛积。\n",
    "  相比之下，点积、矩阵-向量乘积和矩阵-矩阵乘积\n",
    "  不是元素操作，并且通常返回形状不同于操作数的对象。\n",
    "* 与哈达玛积相比，矩阵-矩阵乘积\n",
    "  计算时间要长得多（立方而不是平方）。\n",
    "* 范数捕捉向量（或矩阵）的各种大小概念，\n",
    "  通常应用于两个向量的差值\n",
    "  来测量它们之间的距离。\n",
    "* 常见的向量范数包括 $\\ell_1$ 和 $\\ell_2$ 范数，\n",
    "  常见的矩阵范数包括 *谱* 和 *弗罗贝尼乌斯* 范数。\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 证明矩阵的转置的转置等于该矩阵本身：$(\\mathbf{A}^\\top)^\\top = \\mathbf{A}$。\n",
    "1. 给定两个矩阵 $\\mathbf{A}$ 和 $\\mathbf{B}$，证明求和与转置可交换：$\\mathbf{A}^\\top + \\mathbf{B}^\\top = (\\mathbf{A} + \\mathbf{B})^\\top$。\n",
    "1. 给定任意方阵 $\\mathbf{A}$，$\\mathbf{A} + \\mathbf{A}^\\top$ 是否总是对称的？你能仅使用前两题的结果证明这一点吗？\n",
    "1. 我们在本节中定义了形状为 (2, 3, 4) 的张量 `X`。`len(X)` 的输出是什么？在不实现任何代码的情况下写出你的答案，然后用代码检查你的答案。\n",
    "1. 对于任意形状的张量 `X`，`len(X)` 是否总是对应于 `X` 的某个轴的长度？那个轴是什么？\n",
    "1. 运行 `A / A.sum(axis=1)` 并看看会发生什么。你能分析结果吗？\n",
    "1. 在曼哈顿市中心的两点之间旅行时，你需要覆盖的距离是多少，即在坐标上，即在大道和街道上的距离是多少？你可以斜着走吗？\n",
    "1. 考虑一个形状为 (2, 3, 4) 的张量。沿轴 0、1 和 2 的求和输出的形状分别是什么？\n",
    "1. 将具有三个或更多轴的张量传递给 `linalg.norm` 函数并观察其输出。该函数对于任意形状的张量计算什么？\n",
    "1. 考虑三个大型矩阵，比如 $\\mathbf{A} \\in \\mathbb{R}^{2^{10} \\times 2^{16}}$，$\\mathbf{B} \\in \\mathbb{R}^{2^{16} \\times 2^{5}}$ 和 $\\mathbf{C} \\in \\mathbb{R}^{2^{5} \\times 2^{14}}$，初始化为高斯随机变量。你想计算 $\\mathbf{A} \\mathbf{B} \\mathbf{C}$ 的乘积。计算 $(\\mathbf{A} \\mathbf{B}) \\mathbf{C}$ 或 $\\mathbf{A} (\\mathbf{B} \\mathbf{C})$ 在内存占用和速度上有区别吗？为什么？\n",
    "1. 考虑三个大型矩阵，比如 $\\mathbf{A} \\in \\mathbb{R}^{2^{10} \\times 2^{16}}$，$\\mathbf{B} \\in \\mathbb{R}^{2^{16} \\times 2^{5}}$ 和 $\\mathbf{C} \\in \\mathbb{R}^{2^{5} \\times 2^{16}}$。计算 $\\mathbf{A} \\mathbf{B}$ 或 $\\mathbf{A} \\mathbf{C}^\\top$ 在速度上有区别吗？为什么？如果初始化 $\\mathbf{C} = \\mathbf{B}^\\top$ 而不复制内存，会有什么变化？为什么？\n",
    "1. 考虑三个矩阵，比如 $\\mathbf{A}, \\mathbf{B}, \\mathbf{C} \\in \\mathbb{R}^{100 \\times 200}$。通过堆叠 $[\\mathbf{A}, \\mathbf{B}, \\mathbf{C}]$ 构建一个具有三个轴的张量。维度是什么？切出第三个轴的第二个坐标以恢复 $\\mathbf{B}$。检查你的答案是否正确。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0b33af",
   "metadata": {
    "origin_pos": 124,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[讨论](https://discuss.d2l.ai/t/31)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-en",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}