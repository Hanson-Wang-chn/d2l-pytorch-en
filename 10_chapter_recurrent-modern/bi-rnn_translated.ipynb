{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05af2ce8",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 双向循环神经网络\n",
    ":label:`sec_bi_rnn`\n",
    "\n",
    "到目前为止，我们处理的序列学习任务示例一直是语言模型，\n",
    "其中我们的目标是根据序列中的所有先前标记来预测下一个标记。\n",
    "在这种情况下，我们只希望基于左侧上下文进行条件判断，\n",
    "因此标准RNN的单向链接似乎是合适的。\n",
    "然而，在许多其他序列学习任务中，\n",
    "在每个时间步上基于左侧和右侧上下文进行条件判断是完全合理的。\n",
    "例如，词性标注。\n",
    "为什么在评估与给定单词相关的词性时不考虑两个方向的上下文呢？\n",
    "\n",
    "另一个常见的任务——通常作为实际任务之前的预训练练习——是\n",
    "在文本文档中随机遮蔽一些标记，然后训练\n",
    "一个序列模型来预测缺失标记的值。\n",
    "请注意，根据空白后的内容，\n",
    "缺失标记的可能值会显著变化：\n",
    "\n",
    "* 我是`___`。\n",
    "* 我是`___`饿了。\n",
    "* 我是`___`饿了，我可以吃掉半头猪。\n",
    "\n",
    "在第一句中，“快乐”似乎是一个可能的选择。\n",
    "在第二句中，“不”和“非常”看起来是合理的，\n",
    "但“不”似乎与第三句不兼容。\n",
    "\n",
    "\n",
    "幸运的是，一种简单的方法可以将任何单向RNN\n",
    "转换为双向RNN :cite:`Schuster.Paliwal.1997`。\n",
    "我们只需实现两个以相反方向链接在一起的单向RNN层\n",
    "并作用于相同的输入 (:numref:`fig_birnn`)。\n",
    "对于第一个RNN层，\n",
    "第一个输入是$\\mathbf{x}_1$\n",
    "最后一个输入是$\\mathbf{x}_T$，\n",
    "但对于第二个RNN层，\n",
    "第一个输入是$\\mathbf{x}_T$\n",
    "最后一个输入是$\\mathbf{x}_1$。\n",
    "为了生成这个双向RNN层的输出，\n",
    "我们只需将两个底层单向RNN层的相应输出连接起来即可。\n",
    "\n",
    "\n",
    "![双向RNN的架构。](../img/birnn.svg)\n",
    ":label:`fig_birnn`\n",
    "\n",
    "\n",
    "形式上，对于任何时间步$t$，\n",
    "我们考虑一个小批量输入$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$ \n",
    "（样本数量$=n$；每个样本中的输入数量$=d$） \n",
    "并设隐藏层激活函数为$\\phi$。\n",
    "在双向架构中，\n",
    "此时间步的前向和后向隐藏状态分别为$\\overrightarrow{\\mathbf{H}}_t  \\in \\mathbb{R}^{n \\times h}$ \n",
    "和$\\overleftarrow{\\mathbf{H}}_t  \\in \\mathbb{R}^{n \\times h}$，\n",
    "其中$h$是隐藏单元的数量。\n",
    "前向和后向隐藏状态更新如下：\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\overrightarrow{\\mathbf{H}}_t &= \\phi(\\mathbf{X}_t \\mathbf{W}_{\\textrm{xh}}^{(f)} + \\overrightarrow{\\mathbf{H}}_{t-1} \\mathbf{W}_{\\textrm{hh}}^{(f)}  + \\mathbf{b}_\\textrm{h}^{(f)}),\\\\\n",
    "\\overleftarrow{\\mathbf{H}}_t &= \\phi(\\mathbf{X}_t \\mathbf{W}_{\\textrm{xh}}^{(b)} + \\overleftarrow{\\mathbf{H}}_{t+1} \\mathbf{W}_{\\textrm{hh}}^{(b)}  + \\mathbf{b}_\\textrm{h}^{(b)}),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中权重$\\mathbf{W}_{\\textrm{xh}}^{(f)} \\in \\mathbb{R}^{d \\times h}, \\mathbf{W}_{\\textrm{hh}}^{(f)} \\in \\mathbb{R}^{h \\times h}, \\mathbf{W}_{\\textrm{xh}}^{(b)} \\in \\mathbb{R}^{d \\times h}, \\textrm{ 和 } \\mathbf{W}_{\\textrm{hh}}^{(b)} \\in \\mathbb{R}^{h \\times h}$，以及偏置$\\mathbf{b}_\\textrm{h}^{(f)} \\in \\mathbb{R}^{1 \\times h}$ 和 $\\mathbf{b}_\\textrm{h}^{(b)} \\in \\mathbb{R}^{1 \\times h}$ 都是模型参数。\n",
    "\n",
    "接下来，我们将前向和后向隐藏状态\n",
    "$\\overrightarrow{\\mathbf{H}}_t$ 和 $\\overleftarrow{\\mathbf{H}}_t$\n",
    "连接起来，得到用于馈送到输出层的隐藏状态$\\mathbf{H}_t \\in \\mathbb{R}^{n \\times 2h}$。\n",
    "在具有多个隐藏层的深度双向RNN中，\n",
    "这种信息被作为*输入*传递给下一个双向层。\n",
    "最后，输出层计算输出\n",
    "$\\mathbf{O}_t \\in \\mathbb{R}^{n \\times q}$（输出数量$=q$）：\n",
    "\n",
    "$$\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{\\textrm{hq}} + \\mathbf{b}_\\textrm{q}.$$\n",
    "\n",
    "这里，权重矩阵$\\mathbf{W}_{\\textrm{hq}} \\in \\mathbb{R}^{2h \\times q}$ \n",
    "和偏置$\\mathbf{b}_\\textrm{q} \\in \\mathbb{R}^{1 \\times q}$ \n",
    "是输出层的模型参数。\n",
    "虽然从技术上讲，两个方向可以有不同的隐藏单元数，\n",
    "但在实践中很少做出这样的设计选择。\n",
    "我们现在展示一个简单的双向RNN实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee07527f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:39:47.754935Z",
     "iopub.status.busy": "2023-08-18T19:39:47.754598Z",
     "iopub.status.idle": "2023-08-18T19:39:51.281601Z",
     "shell.execute_reply": "2023-08-18T19:39:51.280325Z"
    },
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2aafd5",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "## 从零开始实现\n",
    "\n",
    "要从零开始实现双向RNN，我们可以\n",
    "包含两个单向的`RNNScratch`实例\n",
    "并具有独立的学习参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3282034f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:39:51.287307Z",
     "iopub.status.busy": "2023-08-18T19:39:51.286196Z",
     "iopub.status.idle": "2023-08-18T19:39:51.293977Z",
     "shell.execute_reply": "2023-08-18T19:39:51.293017Z"
    },
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class BiRNNScratch(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.f_rnn = d2l.RNNScratch(num_inputs, num_hiddens, sigma)\n",
    "        self.b_rnn = d2l.RNNScratch(num_inputs, num_hiddens, sigma)\n",
    "        self.num_hiddens *= 2  # The output dimension will be doubled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d59627",
   "metadata": {
    "origin_pos": 9
   },
   "source": [
    "前向和后向RNN的状态是分别更新的，而这两个RNN的输出被连接在一起。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf749cb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:39:51.298781Z",
     "iopub.status.busy": "2023-08-18T19:39:51.297847Z",
     "iopub.status.idle": "2023-08-18T19:39:51.305149Z",
     "shell.execute_reply": "2023-08-18T19:39:51.304220Z"
    },
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "@d2l.add_to_class(BiRNNScratch)\n",
    "def forward(self, inputs, Hs=None):\n",
    "    f_H, b_H = Hs if Hs is not None else (None, None)\n",
    "    f_outputs, f_H = self.f_rnn(inputs, f_H)\n",
    "    b_outputs, b_H = self.b_rnn(reversed(inputs), b_H)\n",
    "    outputs = [torch.cat((f, b), -1) for f, b in zip(\n",
    "        f_outputs, reversed(b_outputs))]\n",
    "    return outputs, (f_H, b_H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf0384a",
   "metadata": {
    "origin_pos": 11
   },
   "source": [
    "## 简洁实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c382cc",
   "metadata": {
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "使用高级API，\n",
    "我们可以更简洁地实现双向RNN。\n",
    "这里我们以GRU模型为例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e70cf71c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:39:51.309028Z",
     "iopub.status.busy": "2023-08-18T19:39:51.308375Z",
     "iopub.status.idle": "2023-08-18T19:39:51.313930Z",
     "shell.execute_reply": "2023-08-18T19:39:51.312960Z"
    },
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class BiGRU(d2l.RNN):\n",
    "    def __init__(self, num_inputs, num_hiddens):\n",
    "        d2l.Module.__init__(self)\n",
    "        self.save_hyperparameters()\n",
    "        self.rnn = nn.GRU(num_inputs, num_hiddens, bidirectional=True)\n",
    "        self.num_hiddens *= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b98765b",
   "metadata": {
    "origin_pos": 15
   },
   "source": [
    "## 摘要\n",
    "\n",
    "在双向RNN中，每个时间步的隐藏状态同时由当前时间步之前和之后的数据决定。双向RNN主要用于序列编码和给定双向上下文的观测估计。由于长梯度链的存在，双向RNN的训练成本非常高。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 如果不同方向使用不同数量的隐藏单元，$\\mathbf{H}_t$ 的形状将如何变化？\n",
    "1. 设计一个具有多个隐藏层的双向RNN。\n",
    "1. 一词多义在自然语言中很常见。例如，“bank”这个词在“i went to the bank to deposit cash”和“i went to the bank to sit down”两个上下文中具有不同的含义。我们如何设计一个神经网络模型，使得给定一个上下文序列和一个词时，能够返回该词在正确上下文中的向量表示？处理一词多义时首选哪种类型的神经架构？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aab116",
   "metadata": {
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[讨论](https://discuss.d2l.ai/t/1059)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}