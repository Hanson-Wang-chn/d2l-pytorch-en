{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b57309f",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 现代循环神经网络\n",
    ":label:`chap_modern_rnn`\n",
    "\n",
    "前一章介绍了循环神经网络（RNNs）背后的关键思想。然而，就像卷积神经网络一样，RNN架构也经历了大量的创新，最终形成了几种在实践中证明成功的复杂设计。特别是，最受欢迎的设计具有缓解RNN所面临的著名数值不稳定性问题的机制，例如梯度消失和梯度爆炸。回想一下，在:numref:`chap_rnn`中，我们通过应用一种简单的梯度裁剪启发式方法来处理梯度爆炸问题。尽管这种技巧有效，但它并没有解决梯度消失的问题。\n",
    "\n",
    "在本章中，我们将介绍最成功的序列RNN架构背后的关键思想，这些思想源自两篇论文。第一篇，《长短期记忆》:cite:`Hochreiter.Schmidhuber.1997`，引入了*记忆单元*，这是一种计算单元，取代了网络隐藏层中的传统节点。通过这些记忆单元，网络能够克服早期循环网络在训练中遇到的困难。直观上，记忆单元通过将每个记忆单元内部状态的值沿着递归边以权重1传递许多连续的时间步来避免梯度消失问题。一组乘法门帮助网络不仅决定允许进入记忆状态的输入，还决定记忆状态的内容何时应影响模型的输出。\n",
    "\n",
    "第二篇论文，《双向循环神经网络》:cite:`Schuster.Paliwal.1997`，介绍了一种架构，在该架构中，来自未来（后续时间步骤）和过去（先前时间步骤）的信息都被用来确定序列中任何点的输出。这与之前的网络不同，在那些网络中只有过去的输入可以影响输出。双向RNN已经成为自然语言处理中的序列标注任务以及其他众多任务的主要选择。幸运的是，这两种创新并不相互排斥，并且已成功结合用于音素分类:cite:`Graves.Schmidhuber.2005`和手写识别:cite:`graves2008novel`。\n",
    "\n",
    "本章的前几节将解释LSTM架构、一个称为门控循环单元（GRU）的轻量级版本、双向RNN背后的关键思想以及简要说明如何堆叠RNN层以形成深层RNN。随后，我们将探索RNN在序列到序列任务中的应用，介绍机器翻译以及编码器-解码器架构和束搜索等关键概念。\n",
    "\n",
    ":begin_tab:toc\n",
    " - [lstm](lstm.ipynb)\n",
    " - [gru](gru.ipynb)\n",
    " - [deep-rnn](deep-rnn.ipynb)\n",
    " - [bi-rnn](bi-rnn.ipynb)\n",
    " - [machine-translation-and-dataset](machine-translation-and-dataset.ipynb)\n",
    " - [encoder-decoder](encoder-decoder.ipynb)\n",
    " - [seq2seq](seq2seq.ipynb)\n",
    " - [beam-search](beam-search.ipynb)\n",
    ":end_tab:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}