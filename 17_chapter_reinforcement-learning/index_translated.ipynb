{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13deb4dd",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 强化学习\n",
    ":label:`chap_reinforcement_learning`\n",
    "\n",
    "\n",
    "**Pratik Chaudhari** (*University of Pennsylvania and Amazon*), **Rasool Fakoor** (*Amazon*), and **Kavosh Asadi** (*Amazon*)\n",
    "\n",
    "强化学习（RL）是一系列技术，使我们能够构建可以进行顺序决策的机器学习系统。例如，您从在线零售商购买的新衣服包裹送到家门口需要经过一系列决策，例如，零售商在离您家最近的仓库中找到衣服，将衣服放入盒子中，通过陆路或空运运输箱子，并在城市内将其送到您的家中。沿途有许多变量会影响包裹的递送，例如，仓库中是否有这些衣服，运输箱子花了多长时间，是否在每日送货卡车离开之前到达了您的城市等。关键思想是，在每个阶段，这些我们通常无法控制的变量都会影响未来整个事件序列，例如，如果在仓库中打包箱子时出现延误，零售商可能需要通过空运而不是陆运来确保及时交付。强化学习方法使我们能够在顺序决策问题的每个阶段采取适当的行动，以最终最大化某些效用，例如，及时将包裹送到您手中。\n",
    "\n",
    "这种顺序决策问题在许多其他地方也能看到，例如，在玩[围棋](https://en.wikipedia.org/wiki/Go_(game))时，您当前的走法决定了下一步以及对手的走法是您无法控制的变量... 一系列走法最终决定您是否获胜；Netflix现在向您推荐的电影决定了您观看什么，您是否喜欢这部电影对Netflix来说是未知的，最终一系列电影推荐决定了您对Netflix的满意度。今天，强化学习正被用来为这些问题开发有效的解决方案 :cite:`mnih2013playing,Silver.Huang.Maddison.ea.2016`。强化学习与标准深度学习之间的关键区别在于，在标准深度学习中，训练模型对一个测试数据的预测不会影响对未来测试数据的预测；而在强化学习中，未来的决策（在RL中，决策也称为动作）会受到过去所做决策的影响。\n",
    "\n",
    "在本章中，我们将发展强化学习的基础知识，并获得实现一些流行强化学习方法的实际经验。首先，我们将开发一种称为马尔可夫决策过程（MDP）的概念，这使我们能够思考这样的顺序决策问题。一种称为值迭代的算法将是我们解决强化学习问题的第一洞察，假设我们知道MDP中的不可控变量（在RL中，这些受控变量被称为环境）通常如何表现。使用更通用的值迭代版本，即Q-学习算法，即使我们不一定完全了解环境，我们也能够采取适当的行动。然后，我们将研究如何通过模仿专家的行为来使用深度网络解决强化学习问题。最后，我们将开发一种使用深度网络在未知环境中采取行动的强化学习方法。这些技术构成了当今用于各种实际应用的更高级RL算法的基础，我们将在本章中指出其中的一些应用。\n",
    "\n",
    "![Reinforcement Learning Structure](../img/RL_main.png)\n",
    ":width:`400px`\n",
    ":label:`fig_rl_big`\n",
    "\n",
    ":begin_tab:toc\n",
    " - [mdp](mdp.ipynb)\n",
    " - [value-iter](value-iter.ipynb)\n",
    " - [qlearning](qlearning.ipynb)\n",
    ":end_tab:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}